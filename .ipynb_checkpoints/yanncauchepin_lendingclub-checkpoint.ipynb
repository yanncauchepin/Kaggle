{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a499894c",
   "metadata": {},
   "source": [
    "## Yann Cauchepin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfce03",
   "metadata": {},
   "source": [
    "Hi, here is my documented jupyter notebook which respond to the test request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1b09e",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before even running the following script, please process by:\n",
    "\n",
    "- [ ] Installing the interested librairies. You can comment the next script to not bide your time.\n",
    "\n",
    "- [ ] Replacing dataset path toward your local repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a923e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install pyspark\n",
    "# !pip install catboost\n",
    "# !pip install shap\n",
    "# !pip install torch\n",
    "# !pip install captum\n",
    "# !pip install sklearn\n",
    "# !pip install mapie\n",
    "# !pip install scikit-optimize\n",
    "# !pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b006539",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/LCDataDictionary.xlsx\"\n",
    "data_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/Loan_status_2007-2020Q3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939e91c",
   "metadata": {},
   "source": [
    "Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957f17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(metadata_path, index_col=0)\n",
    "metadata = metadata.iloc[:-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f1740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ddf746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 21:38:28 WARN Utils: Your hostname, yanncauchepincomputer resolves to a loopback address: 127.0.1.1; using 192.168.43.208 instead (on interface wlp2s0)\n",
      "24/06/21 21:38:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/21 21:38:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 2:=====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 2925493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LendingClubDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "print(f\"Number of data: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4751cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata features: 151\n",
      "Data features: 142\n",
      "Unknown data features: ['_c0', 'verification_status_joint', 'total_rev_hi_lim', 'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths', 'sec_app_collections_12_mths_ex_med'] (14)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metadata features: {len(metadata.index)}\")\n",
    "print(f\"Data features: {len(df_spark.columns)}\")\n",
    "\n",
    "outer_features = [feature for feature in df_spark.columns if feature not in metadata.index]\n",
    "\n",
    "print(f\"Unknown data features: {outer_features} ({len(outer_features)})\")\n",
    "df_spark = df_spark.drop(*outer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['grade', 'sub_grade']\n",
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83763e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [feature for feature in df_spark.columns if feature != 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3876cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distincts values: 12 - 4.10e-06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:============================================>           (11 + 3) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         loan_status|  count|\n",
      "+--------------------+-------+\n",
      "|          Fully Paid|1497783|\n",
      "|                NULL|      1|\n",
      "|     In Grace Period|  10028|\n",
      "|Does not meet the...|   1988|\n",
      "|         Charged Off| 362547|\n",
      "|  Late (31-120 days)|  16154|\n",
      "|             Current|1031016|\n",
      "|Does not meet the...|    761|\n",
      "|   Late (16-30 days)|   2719|\n",
      "|             Default|    433|\n",
      "|              Issued|   2062|\n",
      "|            Oct-2015|      1|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:====================================================>   (13 + 1) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b9bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mapping = {\n",
    "    'Fully Paid': 0,\n",
    "    'Charged Off': 1,\n",
    "    'Current': np.nan,\n",
    "    'Late (31-120 days)': np.nan,\n",
    "    'In Grace Period': np.nan,\n",
    "    'Late (16-30 days)': np.nan,\n",
    "    'Issued': np.nan,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': np.nan,\n",
    "    'Does not meet the credit policy. Status:Charged Off': np.nan,\n",
    "    'Default': np.nan,\n",
    "    'Oct-2015': np.nan\n",
    "}\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df_spark = df_spark.withColumn(\"loan_status\", when(df_spark[\"loan_status\"] == \"Fully Paid\", 0)\n",
    "                   .when(df_spark[\"loan_status\"] == \"Charged Off\", 1)\n",
    "                   .otherwise(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a37f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.fillna({col: \"nan\" if df_spark.schema[col].dataType == 'string' else np.nan for col in all_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205dda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distincts values: 3 - 1.03e-06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|loan_status|  count|\n",
      "+-----------+-------+\n",
      "|        0.0|1497783|\n",
      "|        NaN|1065163|\n",
      "|        1.0| 362547|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83528a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('loan_amnt', 'int'),\n",
       " ('funded_amnt', 'int'),\n",
       " ('funded_amnt_inv', 'double'),\n",
       " ('term', 'string'),\n",
       " ('int_rate', 'string'),\n",
       " ('installment', 'double'),\n",
       " ('emp_title', 'string'),\n",
       " ('emp_length', 'string'),\n",
       " ('home_ownership', 'string'),\n",
       " ('annual_inc', 'string'),\n",
       " ('verification_status', 'string'),\n",
       " ('issue_d', 'string'),\n",
       " ('loan_status', 'double'),\n",
       " ('pymnt_plan', 'string'),\n",
       " ('url', 'string'),\n",
       " ('purpose', 'string'),\n",
       " ('title', 'string'),\n",
       " ('zip_code', 'string'),\n",
       " ('addr_state', 'string'),\n",
       " ('dti', 'string'),\n",
       " ('delinq_2yrs', 'double'),\n",
       " ('earliest_cr_line', 'string'),\n",
       " ('fico_range_low', 'string'),\n",
       " ('fico_range_high', 'int'),\n",
       " ('inq_last_6mths', 'int'),\n",
       " ('mths_since_last_delinq', 'int'),\n",
       " ('mths_since_last_record', 'int'),\n",
       " ('open_acc', 'int'),\n",
       " ('pub_rec', 'int'),\n",
       " ('revol_bal', 'int'),\n",
       " ('revol_util', 'string'),\n",
       " ('total_acc', 'string'),\n",
       " ('initial_list_status', 'string'),\n",
       " ('out_prncp', 'string'),\n",
       " ('out_prncp_inv', 'double'),\n",
       " ('total_pymnt', 'double'),\n",
       " ('total_pymnt_inv', 'double'),\n",
       " ('total_rec_prncp', 'double'),\n",
       " ('total_rec_int', 'double'),\n",
       " ('total_rec_late_fee', 'double'),\n",
       " ('recoveries', 'double'),\n",
       " ('collection_recovery_fee', 'double'),\n",
       " ('last_pymnt_d', 'string'),\n",
       " ('last_pymnt_amnt', 'string'),\n",
       " ('next_pymnt_d', 'string'),\n",
       " ('last_credit_pull_d', 'string'),\n",
       " ('last_fico_range_high', 'string'),\n",
       " ('last_fico_range_low', 'int'),\n",
       " ('collections_12_mths_ex_med', 'int'),\n",
       " ('mths_since_last_major_derog', 'int'),\n",
       " ('policy_code', 'int'),\n",
       " ('application_type', 'string'),\n",
       " ('annual_inc_joint', 'string'),\n",
       " ('dti_joint', 'double'),\n",
       " ('acc_now_delinq', 'int'),\n",
       " ('tot_coll_amt', 'int'),\n",
       " ('tot_cur_bal', 'int'),\n",
       " ('open_acc_6m', 'int'),\n",
       " ('open_act_il', 'int'),\n",
       " ('open_il_12m', 'int'),\n",
       " ('open_il_24m', 'int'),\n",
       " ('mths_since_rcnt_il', 'int'),\n",
       " ('total_bal_il', 'int'),\n",
       " ('il_util', 'int'),\n",
       " ('open_rv_12m', 'int'),\n",
       " ('open_rv_24m', 'int'),\n",
       " ('max_bal_bc', 'int'),\n",
       " ('all_util', 'int'),\n",
       " ('inq_fi', 'int'),\n",
       " ('total_cu_tl', 'int'),\n",
       " ('inq_last_12m', 'int'),\n",
       " ('acc_open_past_24mths', 'int'),\n",
       " ('avg_cur_bal', 'int'),\n",
       " ('bc_open_to_buy', 'int'),\n",
       " ('bc_util', 'double'),\n",
       " ('chargeoff_within_12_mths', 'double'),\n",
       " ('delinq_amnt', 'int'),\n",
       " ('mo_sin_old_il_acct', 'int'),\n",
       " ('mo_sin_old_rev_tl_op', 'int'),\n",
       " ('mo_sin_rcnt_rev_tl_op', 'int'),\n",
       " ('mo_sin_rcnt_tl', 'int'),\n",
       " ('mort_acc', 'int'),\n",
       " ('mths_since_recent_bc', 'int'),\n",
       " ('mths_since_recent_bc_dlq', 'int'),\n",
       " ('mths_since_recent_inq', 'int'),\n",
       " ('mths_since_recent_revol_delinq', 'int'),\n",
       " ('num_accts_ever_120_pd', 'int'),\n",
       " ('num_actv_bc_tl', 'int'),\n",
       " ('num_actv_rev_tl', 'int'),\n",
       " ('num_bc_sats', 'int'),\n",
       " ('num_bc_tl', 'int'),\n",
       " ('num_il_tl', 'int'),\n",
       " ('num_op_rev_tl', 'int'),\n",
       " ('num_rev_accts', 'int'),\n",
       " ('num_rev_tl_bal_gt_0', 'int'),\n",
       " ('num_sats', 'int'),\n",
       " ('num_tl_120dpd_2m', 'int'),\n",
       " ('num_tl_30dpd', 'int'),\n",
       " ('num_tl_90g_dpd_24m', 'int'),\n",
       " ('num_tl_op_past_12m', 'int'),\n",
       " ('pct_tl_nvr_dlq', 'double'),\n",
       " ('percent_bc_gt_75', 'double'),\n",
       " ('pub_rec_bankruptcies', 'int'),\n",
       " ('tax_liens', 'int'),\n",
       " ('tot_hi_cred_lim', 'int'),\n",
       " ('total_bal_ex_mort', 'int'),\n",
       " ('total_bc_limit', 'int'),\n",
       " ('total_il_high_credit_limit', 'int'),\n",
       " ('sec_app_open_act_il', 'int'),\n",
       " ('hardship_flag', 'string'),\n",
       " ('hardship_type', 'string'),\n",
       " ('hardship_reason', 'string'),\n",
       " ('hardship_status', 'string'),\n",
       " ('deferral_term', 'int'),\n",
       " ('hardship_amount', 'double'),\n",
       " ('hardship_start_date', 'string'),\n",
       " ('hardship_end_date', 'string'),\n",
       " ('payment_plan_start_date', 'string'),\n",
       " ('hardship_length', 'int'),\n",
       " ('hardship_dpd', 'int'),\n",
       " ('hardship_loan_status', 'string'),\n",
       " ('orig_projected_additional_accrued_interest', 'double'),\n",
       " ('hardship_payoff_balance_amount', 'double'),\n",
       " ('hardship_last_payment_amount', 'double'),\n",
       " ('debt_settlement_flag', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a4684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:============>                                            (3 + 8) / 14]\r",
      "\r",
      "[Stage 41:====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 145983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gb = df_spark.sample(fraction=0.05, seed=1)\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e255b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:================================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 92781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gb = df_gb.dropna(subset=['loan_status'])\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1680e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 21:39:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_gb = df_gb.select(all_features)\n",
    "features_collected_gb = features_gb.collect()\n",
    "target_gb = df_gb.select('loan_status')\n",
    "target_collected_gb = target_gb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d0f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb = np.array([list(feature) for feature in features_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee730a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb = np.array([feature['loan_status'] for feature in target_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2387d119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92781, 125)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d48cb9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92781,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a945bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [feature for (feature, dtype) in df_gb.dtypes if dtype=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6859dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.1319934\ttotal: 223ms\tremaining: 22.1s\n",
      "1:\tlearn: 0.0766225\ttotal: 334ms\tremaining: 16.4s\n",
      "2:\tlearn: 0.0388028\ttotal: 450ms\tremaining: 14.6s\n",
      "3:\tlearn: 0.0303303\ttotal: 519ms\tremaining: 12.4s\n",
      "4:\tlearn: 0.0235605\ttotal: 585ms\tremaining: 11.1s\n",
      "5:\tlearn: 0.0206519\ttotal: 650ms\tremaining: 10.2s\n",
      "6:\tlearn: 0.0195695\ttotal: 729ms\tremaining: 9.68s\n",
      "7:\tlearn: 0.0189217\ttotal: 816ms\tremaining: 9.38s\n",
      "8:\tlearn: 0.0175497\ttotal: 910ms\tremaining: 9.2s\n",
      "9:\tlearn: 0.0175093\ttotal: 981ms\tremaining: 8.83s\n",
      "10:\tlearn: 0.0167146\ttotal: 1.04s\tremaining: 8.46s\n",
      "11:\tlearn: 0.0158007\ttotal: 1.11s\tremaining: 8.16s\n",
      "12:\tlearn: 0.0157843\ttotal: 1.18s\tremaining: 7.87s\n",
      "13:\tlearn: 0.0123135\ttotal: 1.25s\tremaining: 7.68s\n",
      "14:\tlearn: 0.0123132\ttotal: 1.31s\tremaining: 7.41s\n",
      "15:\tlearn: 0.0120829\ttotal: 1.38s\tremaining: 7.22s\n",
      "16:\tlearn: 0.0108746\ttotal: 1.44s\tremaining: 7.04s\n",
      "17:\tlearn: 0.0108745\ttotal: 1.49s\tremaining: 6.78s\n",
      "18:\tlearn: 0.0108745\ttotal: 1.54s\tremaining: 6.58s\n",
      "19:\tlearn: 0.0108743\ttotal: 1.6s\tremaining: 6.4s\n",
      "20:\tlearn: 0.0106082\ttotal: 1.68s\tremaining: 6.3s\n",
      "21:\tlearn: 0.0105858\ttotal: 1.75s\tremaining: 6.2s\n",
      "22:\tlearn: 0.0101376\ttotal: 1.82s\tremaining: 6.11s\n",
      "23:\tlearn: 0.0099657\ttotal: 1.9s\tremaining: 6.02s\n",
      "24:\tlearn: 0.0097971\ttotal: 1.97s\tremaining: 5.92s\n",
      "25:\tlearn: 0.0097971\ttotal: 2.03s\tremaining: 5.79s\n",
      "26:\tlearn: 0.0090301\ttotal: 2.11s\tremaining: 5.71s\n",
      "27:\tlearn: 0.0088808\ttotal: 2.19s\tremaining: 5.62s\n",
      "28:\tlearn: 0.0088807\ttotal: 2.23s\tremaining: 5.45s\n",
      "29:\tlearn: 0.0088807\ttotal: 2.28s\tremaining: 5.32s\n",
      "30:\tlearn: 0.0088693\ttotal: 2.34s\tremaining: 5.2s\n",
      "31:\tlearn: 0.0088505\ttotal: 2.4s\tremaining: 5.11s\n",
      "32:\tlearn: 0.0088505\ttotal: 2.46s\tremaining: 4.99s\n",
      "33:\tlearn: 0.0088279\ttotal: 2.52s\tremaining: 4.9s\n",
      "34:\tlearn: 0.0088216\ttotal: 2.59s\tremaining: 4.8s\n",
      "35:\tlearn: 0.0087373\ttotal: 2.66s\tremaining: 4.73s\n",
      "36:\tlearn: 0.0078413\ttotal: 2.74s\tremaining: 4.66s\n",
      "37:\tlearn: 0.0078412\ttotal: 2.79s\tremaining: 4.55s\n",
      "38:\tlearn: 0.0066885\ttotal: 2.87s\tremaining: 4.49s\n",
      "39:\tlearn: 0.0066885\ttotal: 2.92s\tremaining: 4.39s\n",
      "40:\tlearn: 0.0065157\ttotal: 3s\tremaining: 4.32s\n",
      "41:\tlearn: 0.0064130\ttotal: 3.08s\tremaining: 4.25s\n",
      "42:\tlearn: 0.0063812\ttotal: 3.15s\tremaining: 4.18s\n",
      "43:\tlearn: 0.0063812\ttotal: 3.21s\tremaining: 4.09s\n",
      "44:\tlearn: 0.0063309\ttotal: 3.29s\tremaining: 4.02s\n",
      "45:\tlearn: 0.0062925\ttotal: 3.36s\tremaining: 3.94s\n",
      "46:\tlearn: 0.0061442\ttotal: 3.43s\tremaining: 3.87s\n",
      "47:\tlearn: 0.0061000\ttotal: 3.5s\tremaining: 3.8s\n",
      "48:\tlearn: 0.0060704\ttotal: 3.57s\tremaining: 3.72s\n",
      "49:\tlearn: 0.0060494\ttotal: 3.64s\tremaining: 3.64s\n",
      "50:\tlearn: 0.0060438\ttotal: 3.71s\tremaining: 3.56s\n",
      "51:\tlearn: 0.0056426\ttotal: 3.78s\tremaining: 3.49s\n",
      "52:\tlearn: 0.0055848\ttotal: 3.86s\tremaining: 3.42s\n",
      "53:\tlearn: 0.0055847\ttotal: 3.91s\tremaining: 3.33s\n",
      "54:\tlearn: 0.0055030\ttotal: 3.99s\tremaining: 3.26s\n",
      "55:\tlearn: 0.0054123\ttotal: 4.06s\tremaining: 3.19s\n",
      "56:\tlearn: 0.0050235\ttotal: 4.13s\tremaining: 3.12s\n",
      "57:\tlearn: 0.0047845\ttotal: 4.21s\tremaining: 3.05s\n",
      "58:\tlearn: 0.0047173\ttotal: 4.28s\tremaining: 2.97s\n",
      "59:\tlearn: 0.0047172\ttotal: 4.34s\tremaining: 2.9s\n",
      "60:\tlearn: 0.0046941\ttotal: 4.42s\tremaining: 2.83s\n",
      "61:\tlearn: 0.0046940\ttotal: 4.49s\tremaining: 2.75s\n",
      "62:\tlearn: 0.0046540\ttotal: 4.55s\tremaining: 2.67s\n",
      "63:\tlearn: 0.0046535\ttotal: 4.62s\tremaining: 2.6s\n",
      "64:\tlearn: 0.0046141\ttotal: 4.69s\tremaining: 2.53s\n",
      "65:\tlearn: 0.0045433\ttotal: 4.76s\tremaining: 2.45s\n",
      "66:\tlearn: 0.0044677\ttotal: 4.84s\tremaining: 2.38s\n",
      "67:\tlearn: 0.0044677\ttotal: 4.9s\tremaining: 2.31s\n",
      "68:\tlearn: 0.0043488\ttotal: 4.97s\tremaining: 2.23s\n",
      "69:\tlearn: 0.0042946\ttotal: 5.04s\tremaining: 2.16s\n",
      "70:\tlearn: 0.0042359\ttotal: 5.11s\tremaining: 2.09s\n",
      "71:\tlearn: 0.0042071\ttotal: 5.18s\tremaining: 2.01s\n",
      "72:\tlearn: 0.0041746\ttotal: 5.25s\tremaining: 1.94s\n",
      "73:\tlearn: 0.0041202\ttotal: 5.33s\tremaining: 1.87s\n",
      "74:\tlearn: 0.0041200\ttotal: 5.39s\tremaining: 1.8s\n",
      "75:\tlearn: 0.0040684\ttotal: 5.46s\tremaining: 1.73s\n",
      "76:\tlearn: 0.0040683\ttotal: 5.53s\tremaining: 1.65s\n",
      "77:\tlearn: 0.0040164\ttotal: 5.6s\tremaining: 1.58s\n",
      "78:\tlearn: 0.0039820\ttotal: 5.67s\tremaining: 1.51s\n",
      "79:\tlearn: 0.0039598\ttotal: 5.73s\tremaining: 1.43s\n",
      "80:\tlearn: 0.0039597\ttotal: 5.79s\tremaining: 1.36s\n",
      "81:\tlearn: 0.0039349\ttotal: 5.86s\tremaining: 1.29s\n",
      "82:\tlearn: 0.0039181\ttotal: 5.93s\tremaining: 1.21s\n",
      "83:\tlearn: 0.0038998\ttotal: 5.99s\tremaining: 1.14s\n",
      "84:\tlearn: 0.0038938\ttotal: 6.05s\tremaining: 1.07s\n",
      "85:\tlearn: 0.0038823\ttotal: 6.11s\tremaining: 995ms\n",
      "86:\tlearn: 0.0038823\ttotal: 6.18s\tremaining: 923ms\n",
      "87:\tlearn: 0.0038822\ttotal: 6.23s\tremaining: 850ms\n",
      "88:\tlearn: 0.0038421\ttotal: 6.3s\tremaining: 779ms\n",
      "89:\tlearn: 0.0037379\ttotal: 6.38s\tremaining: 709ms\n",
      "90:\tlearn: 0.0037174\ttotal: 6.46s\tremaining: 639ms\n",
      "91:\tlearn: 0.0037173\ttotal: 6.53s\tremaining: 568ms\n",
      "92:\tlearn: 0.0036843\ttotal: 6.62s\tremaining: 498ms\n",
      "93:\tlearn: 0.0036596\ttotal: 6.68s\tremaining: 427ms\n",
      "94:\tlearn: 0.0036312\ttotal: 6.75s\tremaining: 355ms\n",
      "95:\tlearn: 0.0036311\ttotal: 6.8s\tremaining: 283ms\n",
      "96:\tlearn: 0.0036191\ttotal: 6.87s\tremaining: 212ms\n",
      "97:\tlearn: 0.0036191\ttotal: 6.93s\tremaining: 141ms\n",
      "98:\tlearn: 0.0036137\ttotal: 6.99s\tremaining: 70.6ms\n",
      "99:\tlearn: 0.0035804\ttotal: 7.06s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x72ea34fd6110>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "pool = Pool(data=X_gb, label=y_gb, feature_names=all_features, cat_features=categorical_features)\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=100)\n",
    "catboost_model.fit(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc2cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(catboost_model)\n",
    "shap_values = explainer.shap_values(X_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d8587e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <td>417046.553348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recoveries</th>\n",
       "      <td>149158.378179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <td>135989.706385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>128005.130072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>93815.640670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <td>1.477193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_low</th>\n",
       "      <td>0.305097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_acc_6m</th>\n",
       "      <td>0.073228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verification_status</th>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_flag</th>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature_importance\n",
       "total_rec_prncp            417046.553348\n",
       "recoveries                 149158.378179\n",
       "last_fico_range_low        135989.706385\n",
       "loan_amnt                  128005.130072\n",
       "funded_amnt_inv             93815.640670\n",
       "...                                  ...\n",
       "hardship_loan_status            1.477193\n",
       "fico_range_low                  0.305097\n",
       "open_acc_6m                     0.073228\n",
       "verification_status             0.000302\n",
       "hardship_flag                   0.000125\n",
       "\n",
       "[106 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_over_feature = np.sum(np.abs(shap_values), axis=0)\n",
    "feature_importance = pd.DataFrame(data=sum_over_feature, index=all_features, columns=['feature_importance'])\n",
    "feature_importance = feature_importance[feature_importance['feature_importance']>0]\n",
    "feature_importance = feature_importance.sort_values(by='feature_importance', ascending=False)\n",
    "feature_importance.shape\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39cbd894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>cumulative_sum</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <td>417046.553348</td>\n",
       "      <td>4.170466e+05</td>\n",
       "      <td>0.302733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recoveries</th>\n",
       "      <td>149158.378179</td>\n",
       "      <td>5.662049e+05</td>\n",
       "      <td>0.411007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <td>135989.706385</td>\n",
       "      <td>7.021946e+05</td>\n",
       "      <td>0.509721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>128005.130072</td>\n",
       "      <td>8.301998e+05</td>\n",
       "      <td>0.602640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>93815.640670</td>\n",
       "      <td>9.240154e+05</td>\n",
       "      <td>0.670740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <td>1.477193</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_low</th>\n",
       "      <td>0.305097</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_acc_6m</th>\n",
       "      <td>0.073228</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verification_status</th>\n",
       "      <td>0.000302</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_flag</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature_importance  cumulative_sum      rate\n",
       "total_rec_prncp            417046.553348    4.170466e+05  0.302733\n",
       "recoveries                 149158.378179    5.662049e+05  0.411007\n",
       "last_fico_range_low        135989.706385    7.021946e+05  0.509721\n",
       "loan_amnt                  128005.130072    8.301998e+05  0.602640\n",
       "funded_amnt_inv             93815.640670    9.240154e+05  0.670740\n",
       "...                                  ...             ...       ...\n",
       "hardship_loan_status            1.477193    1.377605e+06  1.000000\n",
       "fico_range_low                  0.305097    1.377605e+06  1.000000\n",
       "open_acc_6m                     0.073228    1.377605e+06  1.000000\n",
       "verification_status             0.000302    1.377605e+06  1.000000\n",
       "hardship_flag                   0.000125    1.377605e+06  1.000000\n",
       "\n",
       "[106 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance['cumulative_sum'] = feature_importance['feature_importance'].cumsum()\n",
    "total_sum = feature_importance['feature_importance'].sum()\n",
    "feature_importance['rate'] = feature_importance['cumulative_sum'] / total_sum\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bdd2694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "selected_features = feature_importance[feature_importance['rate'] < threshold].index\n",
    "selected_features\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d8b87fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_drop = [feature for feature in all_features if feature not in selected_features]\n",
    "features_to_drop\n",
    "len(features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8720722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"lending_club\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dbace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "to_float_udf = udf(to_float, FloatType())\n",
    "df_spark = df_spark.withColumn(\"last_fico_range_high\", to_float_udf(df_spark[\"last_fico_range_high\"]))\n",
    "df_spark = df_spark.withColumn(\"last_pymnt_amnt\", to_float_udf(df_spark[\"last_pymnt_amnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8de72519",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_pymnt_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_pymnt_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_pymnt_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_pymnt_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_pymnt_d LIKE 'May-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_pymnt_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_pymnt_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_pymnt_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_pymnt_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_pymnt_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_pymnt_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_pymnt_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_pymnt_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_pymnt_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_pymnt_d_num', 'last_pymnt_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a41c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_credit_pull_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_credit_pull_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_credit_pull_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_credit_pull_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_credit_pull_d LIKE 'May-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_credit_pull_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_credit_pull_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_credit_pull_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_credit_pull_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_credit_pull_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_credit_pull_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_credit_pull_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_credit_pull_d_num', 'last_credit_pull_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ddc7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(\"last_pymnt_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_pymnt_d_num', 'last_pymnt_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a3645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_float_rate = udf(lambda x: float(x.replace('%', '')) / 100, FloatType())\n",
    "df_spark = df_spark.withColumn('int_rate', convert_to_float_rate(df_spark['int_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab30efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "numerical_selected_features = [feature for (feature, dtype) in df_spark.dtypes if (dtype!='string' and feature !='loan_status')]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_selected_features,\n",
    "    outputCols=numerical_selected_features\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "df_spark = imputer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a751f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan\n",
    "# for feature, dtype in df_spark.dtypes:\n",
    "#     print(\"====================================\")\n",
    "#     print(f\"FEATURE: {feature}\")\n",
    "#     if dtype=='string':\n",
    "#         value_counts = df_spark.groupBy(feature).count()\n",
    "#         value_rate = value_counts.count() / df_spark.count()\n",
    "#         print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "#         value_counts.show()\n",
    "#     nan_count = df_spark.filter(df_spark[feature].isNull() | isnan(df_spark[feature])).count()\n",
    "#     nan_rate = nan_count / df_spark.count()\n",
    "#     print(f\"{nan_count} NaN - {nan_rate:.2e} %\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f79120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 14518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:============================================>           (11 + 3) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data: 9247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:====================================================>   (13 + 1) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df_spark.sample(fraction=0.005, seed=1)\n",
    "print(f\"Number of data: {df.count()}\")\n",
    "df_labeled = df.dropna(subset=['loan_status'])\n",
    "print(f\"Number of labeled data: {df_labeled.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b745349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN data: 5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:================================================>       (12 + 2) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "df_nan = df.filter(isnan(df['loan_status']))\n",
    "print(f\"Number of NaN data: {df_nan.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "756ac9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in list(df_spark.columns) if feature != 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8eb7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_collected = df_labeled.select(features).collect()\n",
    "target_collected = df_labeled.select('loan_status').collect()\n",
    "X_labeled = np.array([list(feature) for feature in features_collected])\n",
    "y_labeled = np.array([feature['loan_status'] for feature in target_collected])\n",
    "y_labeled = y_labeled.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6636f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_collected = df_nan.select(features).collect()\n",
    "X_unlabeled = np.array([list(feature) for feature in features_collected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b383f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = np.unique(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3b5568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from captum.attr import IntegratedGradients\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from mapie.classification import MapieClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_sizes, activation_name, p_dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        if isinstance(self.hidden_layer_sizes, str):\n",
    "            self.hidden_layer_sizes = eval(self.hidden_layer_sizes)\n",
    "        self.activation_name = activation_name\n",
    "        self.p_dropout = p_dropout\n",
    "        if activation_name == \"Relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_name == \"Sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation_name == \"Softmax\":\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        elif activation_name == \"Tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_name == \"Leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation: {self.activation_name}')\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_size, self.hidden_layer_sizes[0]))\n",
    "        layers.append(self.activation)\n",
    "        for i in range(len(self.hidden_layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(self.hidden_layer_sizes[i], self.hidden_layer_sizes[i + 1]))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.Dropout(p=self.p_dropout))\n",
    "        layers.append(nn.Linear(self.hidden_layer_sizes[-1], self.output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward_proba(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.float()\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MLPEstimatorSklearn():\n",
    "    def __init__(self, **params):\n",
    "        self.input_size = params.get(\"input_size\")\n",
    "        self.output_size = params.get(\"output_size\")\n",
    "        self.hidden_layer_sizes = params.get(\"hidden_layer_sizes\", (60, 60))\n",
    "        self.activation_name = params.get(\"activation_name\", \"Relu\")\n",
    "        self.loss = params.get(\"loss\", \"binary_cross_entropy\")\n",
    "        self.optimizer_name = params.get(\"optimizer_name\", \"Adam\")\n",
    "        self.learning_rate = params.get(\"learning_rate\", 1e-3)\n",
    "        self.batch_size = params.get(\"batch_size\", 50)\n",
    "        self.weight_decay = params.get(\"weight_decay\", 0)\n",
    "        self.p_dropout = params.get(\"p_dropout\", 0.2)\n",
    "        self.early_stopping = params.get(\"early_stopping\", True)\n",
    "        self.epochs = params.get(\"epochs\", 200)\n",
    "        self.patience = params.get(\"patience\", 10)\n",
    "        self.verbose = params.get(\"verbose\", True)\n",
    "        self.classes_ = classes_\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = MLP(self.input_size, self.output_size, self.hidden_layer_sizes, self.activation_name, self.p_dropout).to(self.device)\n",
    "\n",
    "        if self.loss == \"binary_cross_entropy\":\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss: {self.loss}\")\n",
    "\n",
    "        if self.optimizer_name == \"SGD\":\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9, weight_decay=self.weight_decay)\n",
    "        elif self.optimizer_name == \"Adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
    "            \n",
    "    def next_batch(self, inputs, targets, batchSize):\n",
    "        inputs_tensor = torch.from_numpy(inputs).float()\n",
    "        targets_tensor = torch.from_numpy(targets).float().unsqueeze(1)\n",
    "        for i in range(0, inputs_tensor.shape[0], batchSize):\n",
    "            yield (inputs_tensor[i:i + batchSize], targets_tensor[i:i + batchSize])\n",
    "\n",
    "    def augment_data(self, X_unlabeled, noise_level=0.1):\n",
    "        noise = noise_level * torch.randn_like(X_unlabeled)\n",
    "        return X_unlabeled + noise\n",
    "            \n",
    "    def fit(self, X, y, X_unlabeled=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        running_losses_1 = list()\n",
    "        if self.early_stopping:\n",
    "            best_loss = float('inf')\n",
    "            count = 0\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1 = tqdm(range(self.epochs), desc=\"Supervised training ; epochs\", unit=\"epoch\")\n",
    "        else:\n",
    "            epoch_iterator_1 = range(self.epochs)\n",
    "        for epoch in epoch_iterator_1:\n",
    "            samples = 0\n",
    "            train_loss = 0.0\n",
    "            self.model.train(True)\n",
    "            for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                batchX = batchX.to(self.device)\n",
    "                batchY = batchY.to(self.device)\n",
    "                batchY.requires_grad = True\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batchX)\n",
    "                loss = self.criterion(outputs, batchY)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                samples += batchY.size(0)\n",
    "            running_loss = train_loss / samples\n",
    "            running_losses_1.append(running_loss)\n",
    "            if self.verbose:\n",
    "                epoch_iterator_1.set_postfix(train_loss=running_loss)\n",
    "            if self.early_stopping:\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    count = 0\n",
    "                else:\n",
    "                    count += 1\n",
    "                if count >= self.patience:\n",
    "                    break\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1.close()\n",
    "        self.running_losses_1 = [loss for loss in running_losses_1 if loss <= best_loss]\n",
    "        if X_unlabeled is not None:\n",
    "            best_loss = float('inf')\n",
    "            running_losses_2 = list()\n",
    "            X_unlabeled = torch.from_numpy(X_unlabeled).float()\n",
    "            augmented_X_unlabeled = self.augment_data(X_unlabeled)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = self.model(X_unlabeled)\n",
    "                pseudo_labels = (pseudo_labels > 0.5).float().squeeze()\n",
    "            \n",
    "            X_combined = torch.cat((torch.from_numpy(X).float(), augmented_X_unlabeled.cpu()), 0).to(self.device)\n",
    "            y_combined = torch.cat((torch.from_numpy(y).float().squeeze(), pseudo_labels), 0).to(self.device)\n",
    "\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2 = tqdm(range(self.epochs), desc=\"Semi-supervised training ; epochs\", unit=\"epoch\")\n",
    "            else:\n",
    "                epoch_iterator_2 = range(self.epochs)\n",
    "            for epoch in epoch_iterator_2:\n",
    "                samples = 0\n",
    "                train_loss = 0.0\n",
    "                self.model.train(True)\n",
    "                dataset = TensorDataset(X_combined, y_combined)\n",
    "                for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                    batchX = batchX.to(self.device)\n",
    "                    batchY = batchY.to(self.device)\n",
    "                    batchY.requires_grad = True\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(batchX)\n",
    "                    loss = self.criterion(outputs, batchY)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                    samples += batchY.size(0)\n",
    "                running_loss = train_loss / samples\n",
    "                running_losses_2.append(running_loss)\n",
    "                if self.verbose:\n",
    "                    epoch_iterator_2.set_postfix(train_loss=running_loss)\n",
    "                if self.early_stopping:\n",
    "                    if running_loss < best_loss:\n",
    "                        best_loss = running_loss\n",
    "                        count = 0\n",
    "                    else:\n",
    "                        count += 1\n",
    "                    if count >= self.patience:\n",
    "                        break\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2.close()\n",
    "            self.running_losses_2 = [loss for loss in running_losses_2 if loss <= best_loss]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_pred = self.model.forward(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            y_pred = y_pred.detach().numpy()\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_proba = self.model.forward_proba(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_proba = y_proba.cpu().detach().numpy().astype(float)\n",
    "        else:\n",
    "            y_proba = y_proba.detach().numpy().astype(float)\n",
    "        y_proba = y_proba.squeeze().astype(np.float32)\n",
    "        return np.column_stack((1 - y_proba, y_proba))\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            \"input_size\": self.input_size,\n",
    "            \"output_size\": self.output_size,\n",
    "            \"hidden_layer_sizes\": self.hidden_layer_sizes,\n",
    "            \"activation_name\": self.activation_name,\n",
    "            \"loss\": self.loss,\n",
    "            \"optimizer_name\": self.optimizer_name,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"p_dropout\": self.p_dropout,\n",
    "            \"early_stopping\": self.early_stopping,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"patience\": self.patience,\n",
    "            \"verbose\": self.verbose\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def get_mlp(self):\n",
    "        return self.model\n",
    "    \n",
    "class MLPBinaryClassifier():\n",
    "    def __init__(self, X, y, split_test, X_unlabeled=None, **params):\n",
    "        self.model = MLPEstimatorSklearn(**params)\n",
    "        self.X = X\n",
    "        self.X_unlabeled = X_unlabeled\n",
    "        self.y = y\n",
    "        \n",
    "        self.y = MLPBinaryClassifier.float_to_class(self.y).ravel()\n",
    "        \n",
    "        self.split_test = split_test\n",
    "        self.split_data()\n",
    "        \n",
    "        self.standardize(self.X_train_cal)\n",
    "        self.X_train_standard = self.standardize_X(self.X_train)\n",
    "        self.X_cal_standard = self.standardize_X(self.X_cal)\n",
    "        if isinstance(self.X_unlabeled, np.ndarray):\n",
    "            self.X_unlabeled_standard = self.standardize_X(self.X_unlabeled)\n",
    "        else :\n",
    "            self.X_unlabeled_standard = None\n",
    "        self.y_train_standard = self.y_train\n",
    "        self.y_cal_standard = self.y_cal.reshape(-1,1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def float_to_class(y):\n",
    "        threshold = 0.5\n",
    "        return (y >= threshold).astype(int)\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.X_train_cal, self.X_test, self.y_train_cal, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=self.split_test, shuffle=True, random_state=1, stratify=self.y)\n",
    "        self.X_train, self.X_cal, self.y_train, self.y_cal = train_test_split(\n",
    "            self.X_train_cal, self.y_train_cal, test_size=0.25, shuffle=True, random_state=1, stratify=self.y_train_cal)\n",
    "\n",
    "    def standardize(self, X):\n",
    "        self.scaler_X_train = StandardScaler()\n",
    "        self.scaler_X_train.fit(X)         \n",
    "\n",
    "\n",
    "    def standardize_X(self, X):\n",
    "        X_new = self.scaler_X_train.transform(X)\n",
    "        return X_new\n",
    "    \n",
    "\n",
    "\n",
    "    def bayes_search(self, param_bayes, n_iter, n_points=1, cv=5, scoring='accuracy',\n",
    "                 verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        bayes_search = BayesSearchCV(self.model, param_bayes, n_iter=n_iter,\n",
    "                                     n_points=n_points, cv=cv, scoring=scoring,\n",
    "                                     verbose=verbose, return_train_score=True,\n",
    "                                     n_jobs=n_jobs, random_state=1)\n",
    "        bayes_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "        self.model = bayes_search.best_estimator_\n",
    "        print(f'Best hyperparameters bayes search : {bayes_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def randomized_search(self, param_randomized, n_iter, cv=5, scoring='accuracy',\n",
    "                      verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        randomized_search = RandomizedSearchCV(self.model, param_randomized,\n",
    "                                               n_iter=n_iter, cv=cv, scoring=scoring,\n",
    "                                               verbose=verbose, return_train_score=True,\n",
    "                                               n_jobs=n_jobs, random_state=1)\n",
    "        randomized_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(randomized_search.cv_results_)\n",
    "        self.model = randomized_search.best_estimator_\n",
    "        print(f'Best hyperparameters randomized search : {randomized_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def fit(self, method=\"lac\"):\n",
    "        self.model.fit(self.X_train_standard, self.y_train_standard, self.X_unlabeled_standard)\n",
    "        self.model_mapie = MapieClassifier(estimator=self.model, cv=\"prefit\", method=method)\n",
    "        self.model_mapie.fit(self.X_cal_standard, self.y_cal_standard)\n",
    "\n",
    "    def predict(self, X, alpha=0.05):\n",
    "        X_standard = self.standardize_X(X)\n",
    "        y_pred, y_ps = self.model_mapie.predict(X_standard, alpha=alpha)\n",
    "        return y_pred, y_ps\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(metric, y_true, y_pred):\n",
    "        y_pred = MLPBinaryClassifier.float_to_class(y_pred)\n",
    "        accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "        precision = metrics.precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "        metrics_dict = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "        }\n",
    "        if metric != 'all':\n",
    "            metrics_dict = {metric: metrics_dict[metric]}\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "    def model_performance(self, metric='all'):\n",
    "        y_pred_train, _ = self.predict(self.X_train)\n",
    "        scores_train = MLPBinaryClassifier.compute_metrics(metric, self.y_train, y_pred_train)\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, self.y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_train.items():\n",
    "            data['Train Set - '+key] = [value]\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def model_performance_test(self, X_test, y_test, metric='all'):\n",
    "        y_pred_test, _ = self.predict(X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def receiver_operating_characteristics(self):\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(self.y_test, y_pred_test)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.title(\"Receiver Operating Characteristics\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.show()\n",
    "\n",
    "    def compute_integrated_gradients(self, X, baseline=None, steps=50):\n",
    "        def preprocess_input(X):\n",
    "            return torch.tensor(X, dtype=torch.float32)\n",
    "        input_tensor = preprocess_input(X)\n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(input_tensor)\n",
    "        integrated_gradients = IntegratedGradients(self.model.get_mlp())\n",
    "        attributions = integrated_gradients.attribute(input_tensor, baseline, target=0, n_steps=steps)\n",
    "        attributions_df = pd.DataFrame(attributions.cpu().detach().numpy(), columns=features)\n",
    "        avg_attributions = attributions_df.mean(axis=0)\n",
    "        avg_abs_attributions = avg_attributions.abs()\n",
    "        def custom_minmax_scaler(data, feature_range=(0, 100)):\n",
    "            min_val = np.min(data)\n",
    "            max_val = np.max(data)\n",
    "            if max_val - min_val == 0:\n",
    "                return np.zeros_like(data) if feature_range[0] == 0 else np.full_like(data, feature_range[0])\n",
    "            scale = (feature_range[1] - feature_range[0]) / (max_val - min_val)\n",
    "            min_range = feature_range[0]\n",
    "            scaled_data = scale * (data - min_val) + min_range\n",
    "            return scaled_data\n",
    "        normalized_data = custom_minmax_scaler(avg_abs_attributions.values.reshape(-1, 1)).astype(float)\n",
    "        np.set_printoptions(suppress=True, precision=2)\n",
    "        normalized_attributions = pd.DataFrame(normalized_data, columns=['attribution'], index=features)\n",
    "        sorted_attributions = normalized_attributions.sort_values(by=\"attribution\", ascending=False)\n",
    "        return sorted_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab236126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "from skopt.space import Real\n",
    "\n",
    "params = {    \n",
    "    \"init\" : {\n",
    "        \"input_size\" : len(features),\n",
    "        \"output_size\" : 1,\n",
    "        \"hidden_layer_sizes\" : (60,60),\n",
    "        \"activation_name\" : \"Relu\",\n",
    "        \"optimizer_name\" : \"Adam\",\n",
    "        \"learning_rate\" : 1e-3,\n",
    "        \"batch_size\" : 50,\n",
    "        \"weight_decay\" : 0,\n",
    "        \"p_dropout\" : 0.3,\n",
    "        \"loss\" : \"binary_cross_entropy\",\n",
    "        \"early_stopping\" : True,\n",
    "        \"epochs\" : 200,\n",
    "        \"patience\" : 10,\n",
    "        \"verbose\" : True\n",
    "    },\n",
    "    \"randomized\": {\n",
    "        \"hidden_layer_sizes\" : [(10,),(50,),(100,),(10,10),(50,50),(60,60),(100,50),(100,100),(100,50,25)],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : loguniform(1e-5, 1),\n",
    "        \"p_dropout\" : uniform(0, 0.4)   \n",
    "    },\n",
    "    \"bayes\": {\n",
    "        \"hidden_layer_sizes\" : [\"(10,)\",\"(50,)\",\"(100,)\",\"(10,10)\",\"(50,50)\",\"(60,60)\",\"(100,50)\",\"(100,100)\",\"(100,50,25)\"],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : Real(1e-5, 1, prior='log-uniform'),\n",
    "        \"p_dropout\" : Real(0, 0.4, prior='uniform')\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mlp = MLPBinaryClassifier(X=X_labeled, y=y_labeled, X_unlabeled=X_unlabeled, split_test=0.2, **params[\"init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02818466",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "n_points=1\n",
    "cv=5\n",
    "scoring='accuracy'\n",
    "verbose=3\n",
    "n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5845bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp.bayes_search(\n",
    "#     param_bayes=params['bayes'],\n",
    "#     n_iter=n_iter,\n",
    "#     n_points=n_points,\n",
    "#     cv=cv,\n",
    "#     scoring=scoring,\n",
    "#     n_jobs=n_jobs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d63527ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  82%|████████▏ | 164/200 [00:09<00:02, 17.35epoch/s, train_loss=2.24e-6]\n",
      "Supervised training ; epochs:  84%|████████▍ | 168/200 [00:09<00:01, 17.17epoch/s, train_loss=1.75e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.997) total time=   9.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Supervised training ; epochs:  92%|█████████▎| 185/200 [00:09<00:00, 17.66epoch/s, train_loss=1.43e-6]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00255]\r",
      "Supervised training ; epochs:  84%|████████▍ | 168/200 [00:09<00:01, 17.17epoch/s, train_loss=2.31e-6]\r",
      "Supervised training ; epochs:  33%|███▎      | 66/200 [00:09<00:16,  8.10epoch/s, train_loss=1.83e-5]\r",
      "Supervised training ; epochs:  34%|███▎      | 67/200 [00:09<00:15,  8.32epoch/s, train_loss=1.83e-5]\r",
      "Supervised training ; epochs:  36%|███▌      | 72/200 [00:09<00:20,  6.36epoch/s, train_loss=8.33e-6]\r",
      "Supervised training ; epochs:  36%|███▋      | 73/200 [00:09<00:22,  5.63epoch/s, train_loss=8.33e-6]\r",
      "Supervised training ; epochs:  92%|█████████▎| 185/200 [00:09<00:00, 17.66epoch/s, train_loss=1.37e-6]\r",
      "Supervised training ; epochs:  93%|█████████▎| 186/200 [00:09<00:00, 19.02epoch/s, train_loss=1.37e-6]\n",
      "\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00179]\r",
      "Supervised training ; epochs:   1%|          | 2/200 [00:00<00:13, 14.72epoch/s, train_loss=0.00179]\r",
      "Supervised training ; epochs:  38%|███▊      | 75/200 [00:09<00:14,  8.82epoch/s, train_loss=1.51e-5]\r",
      "Supervised training ; epochs:  38%|███▊      | 76/200 [00:09<00:14,  8.81epoch/s, train_loss=1.51e-5]\r",
      "Supervised training ; epochs:  39%|███▉      | 78/200 [00:09<00:19,  6.25epoch/s, train_loss=1.02e-5]\r",
      "Supervised training ; epochs:  40%|███▉      | 79/200 [00:09<00:19,  6.35epoch/s, train_loss=1.02e-5]\r",
      "Supervised training ; epochs:  84%|████████▍ | 168/200 [00:09<00:01, 17.17epoch/s, train_loss=1.65e-6]\r",
      "Supervised training ; epochs:  85%|████████▌ | 170/200 [00:09<00:01, 16.89epoch/s, train_loss=1.65e-6]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]\r",
      "Supervised training ; epochs:  34%|███▎      | 67/200 [00:09<00:15,  8.32epoch/s, train_loss=1.49e-5]\r",
      "Supervised training ; epochs:  34%|███▍      | 68/200 [00:09<00:15,  8.41epoch/s, train_loss=1.49e-5]\r",
      "Supervised training ; epochs:  40%|████      | 81/200 [00:09<00:17,  6.92epoch/s, train_loss=7.77e-6]\r",
      "Supervised training ; epochs:  40%|████      | 81/200 [00:09<00:14,  8.20epoch/s, train_loss=7.77e-6]\n",
      "\r",
      "Supervised training ; epochs:   1%|          | 2/200 [00:00<00:13, 14.72epoch/s, train_loss=0.00109]\r",
      "Supervised training ; epochs:  85%|████████▌ | 170/200 [00:09<00:01, 16.89epoch/s, train_loss=2.21e-6]\r",
      "Supervised training ; epochs:  38%|███▊      | 76/200 [00:09<00:14,  8.81epoch/s, train_loss=1.29e-5]\r",
      "Supervised training ; epochs:  38%|███▊      | 77/200 [00:09<00:13,  8.81epoch/s, train_loss=1.29e-5]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00254]\r",
      "Supervised training ; epochs:  36%|███▋      | 73/200 [00:09<00:22,  5.63epoch/s, train_loss=9.31e-6]\r",
      "Supervised training ; epochs:  37%|███▋      | 74/200 [00:09<00:21,  5.89epoch/s, train_loss=9.31e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.996) total time=   9.8s\n",
      "[CV 4/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=1.000, test=0.997) total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00178]\r",
      "Supervised training ; epochs:  40%|███▉      | 79/200 [00:09<00:19,  6.35epoch/s, train_loss=7.01e-6]\r",
      "Supervised training ; epochs:  40%|███▉      | 79/200 [00:09<00:15,  7.95epoch/s, train_loss=7.01e-6]\n",
      "\r",
      "Supervised training ; epochs:  34%|███▍      | 68/200 [00:09<00:15,  8.41epoch/s, train_loss=2.89e-5]\r",
      "Supervised training ; epochs:  34%|███▍      | 69/200 [00:09<00:15,  8.50epoch/s, train_loss=2.89e-5]\r",
      "Supervised training ; epochs:  85%|████████▌ | 170/200 [00:09<00:01, 16.89epoch/s, train_loss=1.69e-6]\r",
      "Supervised training ; epochs:  86%|████████▌ | 172/200 [00:09<00:01, 15.17epoch/s, train_loss=1.69e-6]\r",
      "Supervised training ; epochs:   1%|          | 2/200 [00:00<00:13, 14.72epoch/s, train_loss=0.000658]\r",
      "Supervised training ; epochs:   2%|▏         | 4/200 [00:00<00:17, 11.42epoch/s, train_loss=0.000658]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00116]\r",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:09, 21.85epoch/s, train_loss=0.00116]\r",
      "Supervised training ; epochs:  38%|███▊      | 77/200 [00:09<00:13,  8.81epoch/s, train_loss=7.49e-6]\r",
      "Supervised training ; epochs:  39%|███▉      | 78/200 [00:09<00:14,  8.69epoch/s, train_loss=7.49e-6]\r",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:09, 21.85epoch/s, train_loss=0.000737]\r",
      "Supervised training ; epochs:  86%|████████▌ | 172/200 [00:10<00:01, 15.17epoch/s, train_loss=1.52e-6]\r",
      "Supervised training ; epochs:   2%|▏         | 4/200 [00:00<00:17, 11.42epoch/s, train_loss=0.000462]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]\r",
      "Supervised training ; epochs:  37%|███▋      | 74/200 [00:10<00:21,  5.89epoch/s, train_loss=7.9e-6] \r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]\r",
      "Supervised training ; epochs:  38%|███▊      | 75/200 [00:10<00:21,  5.77epoch/s, train_loss=7.9e-6]\r",
      "Supervised training ; epochs:   2%|▏         | 4/200 [00:00<00:17, 11.42epoch/s, train_loss=0.00035] \r",
      "Supervised training ; epochs:   3%|▎         | 6/200 [00:00<00:14, 13.83epoch/s, train_loss=0.00035]\r",
      "Supervised training ; epochs:  86%|████████▌ | 172/200 [00:10<00:01, 15.17epoch/s, train_loss=1.71e-6]\r",
      "Supervised training ; epochs:  87%|████████▋ | 174/200 [00:10<00:01, 15.52epoch/s, train_loss=1.71e-6]\r",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:09, 21.85epoch/s, train_loss=0.00051] \r",
      "Supervised training ; epochs:  39%|███▉      | 78/200 [00:10<00:14,  8.69epoch/s, train_loss=1.6e-5] \r",
      "Supervised training ; epochs:  40%|███▉      | 79/200 [00:10<00:13,  8.65epoch/s, train_loss=1.6e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=1.000, test=0.998) total time=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs: 100%|██████████| 200/200 [00:11<00:00, 17.22epoch/s, train_loss=1.02e-6]\n",
      "Supervised training ; epochs:  15%|█▌        | 30/200 [00:02<00:11, 14.36epoch/s, train_loss=4.86e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.999) total time=  11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  48%|████▊     | 96/200 [00:12<00:13,  7.55epoch/s, train_loss=1.13e-5]]\n",
      "Supervised training ; epochs:   2%|▎         | 5/200 [00:01<00:46,  4.22epoch/s, train_loss=0.000617]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=1.000, test=0.997) total time=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  56%|█████▌    | 111/200 [00:13<00:11,  7.96epoch/s, train_loss=1.18e-5]\n",
      "Supervised training ; epochs:  31%|███       | 62/200 [00:04<00:11, 12.12epoch/s, train_loss=1.86e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=0.998, test=0.995) total time=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  60%|█████▉    | 119/200 [00:17<00:12,  6.66epoch/s, train_loss=4.18e-5]\n",
      "Supervised training ; epochs:  72%|███████▏  | 143/200 [00:08<00:02, 19.95epoch/s, train_loss=1.67e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=0.999, test=0.996) total time=  17.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  82%|████████▏ | 164/200 [00:10<00:02, 15.17epoch/s, train_loss=1.95e-6]\n",
      "Supervised training ; epochs:  92%|█████████▏| 183/200 [00:10<00:00, 17.14epoch/s, train_loss=1.18e-6]\n",
      "Supervised training ; epochs:   0%|          | 1/200 [00:00<00:26,  7.47epoch/s, train_loss=0.00589]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.996) total time=  10.8s\n",
      "[CV 5/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.999) total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  22%|██▏       | 44/200 [00:05<00:20,  7.58epoch/s, train_loss=4.19e-5]]\n",
      "Supervised training ; epochs:  12%|█▎        | 25/200 [00:03<00:22,  7.90epoch/s, train_loss=8.44e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Sigmoid, alpha=0.03162277660168379, batch_size=80, hidden_layer_sizes=(100, 50), learning_rate=0.003584739875476995, optimizer_name=Adam, p_dropout=0.357842665401539, weight_decay=2.6620797194541587e-05;, score=(train=0.999, test=0.995) total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  38%|███▊      | 76/200 [00:16<00:27,  4.48epoch/s, train_loss=4.21e-5]]\n",
      "Supervised training ; epochs:   0%|          | 1/200 [00:00<00:25,  7.72epoch/s, train_loss=0.00537]] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=0.999, test=0.995) total time=  17.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  30%|███       | 60/200 [00:14<00:34,  4.11epoch/s, train_loss=7.06e-5] \n",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:07, 27.05epoch/s, train_loss=0.000732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=0.998, test=0.996) total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  34%|███▎      | 67/200 [00:17<00:33,  3.93epoch/s, train_loss=5.56e-5] \n",
      "Supervised training ; epochs:  20%|█▉        | 39/200 [00:07<00:31,  5.17epoch/s, train_loss=4.44e-5]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=1.000, test=0.997) total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  38%|███▊      | 76/200 [00:21<00:35,  3.51epoch/s, train_loss=5.7e-6]  \n",
      "Supervised training ; epochs:  43%|████▎     | 86/200 [00:20<00:26,  4.28epoch/s, train_loss=1.77e-5]]\n",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:08, 23.28epoch/s, train_loss=0.00106]6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=1.000, test=0.996) total time=  21.7s\n",
      "[CV 3/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=0.998, test=0.995) total time=  20.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  87%|████████▋ | 174/200 [00:06<00:00, 27.72epoch/s, train_loss=1.64e-6]\n",
      "Supervised training ; epochs:  22%|██▎       | 45/200 [00:01<00:07, 21.95epoch/s, train_loss=2.61e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Leaky_relu, alpha=0.31622776601683794, batch_size=430, hidden_layer_sizes=(100, 50, 25), learning_rate=0.0001972606689184097, optimizer_name=SGD, p_dropout=0.26866163896885376, weight_decay=0.0011453530979564342;, score=(train=1.000, test=0.997) total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  49%|████▉     | 98/200 [00:16<00:17,  5.97epoch/s, train_loss=2.98e-6]]\n",
      "Supervised training ; epochs:  89%|████████▉ | 178/200 [00:05<00:00, 29.85epoch/s, train_loss=1.89e-6]\n",
      "Supervised training ; epochs:  54%|█████▍    | 108/200 [00:17<00:15,  6.11epoch/s, train_loss=1.45e-5] \n",
      "Supervised training ; epochs:  68%|██████▊   | 135/200 [00:04<00:02, 29.63epoch/s, train_loss=4.55e-6]\n",
      "Supervised training ; epochs:  86%|████████▌ | 172/200 [00:06<00:01, 25.30epoch/s, train_loss=1.69e-6]\n",
      "Supervised training ; epochs: 100%|██████████| 200/200 [00:07<00:00, 28.55epoch/s, train_loss=1.14e-6]\n",
      "Supervised training ; epochs:  42%|████▎     | 85/200 [00:15<00:20,  5.61epoch/s, train_loss=8.96e-6]\n",
      "Supervised training ; epochs:  41%|████      | 82/200 [00:12<00:17,  6.68epoch/s, train_loss=2.29e-5]\n",
      "Supervised training ; epochs: 100%|█| 200/200 [00:04<00:00, 47.58epo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters randomized search : {'activation_name': 'Softmax', 'alpha': 0.01, 'batch_size': 260, 'hidden_layer_sizes': (100,), 'learning_rate': 0.034588581370567514, 'optimizer_name': 'SGD', 'p_dropout': 0.2740878001587038, 'weight_decay': 0.00010525948689799706}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation_name</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer_name</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.878509</td>\n",
       "      <td>2.959863</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>Leaky_relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>90</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996755</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.000981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.470719</td>\n",
       "      <td>0.772705</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>Softmax</td>\n",
       "      <td>0.01</td>\n",
       "      <td>260</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997656</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.085190</td>\n",
       "      <td>2.496287</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>0.215443</td>\n",
       "      <td>50</td>\n",
       "      <td>(100, 100)</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996214</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997747</td>\n",
       "      <td>0.997747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998963</td>\n",
       "      <td>0.001024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.467711</td>\n",
       "      <td>4.224989</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>80</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996755</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998648</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.123708</td>\n",
       "      <td>0.866223</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>Leaky_relu</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>430</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997296</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      12.878509      2.959863         0.003221        0.000791   \n",
       "1      10.470719      0.772705         0.002413        0.000233   \n",
       "2      18.085190      2.496287         0.002672        0.000599   \n",
       "3      13.467711      4.224989         0.002278        0.000755   \n",
       "4       6.123708      0.866223         0.001796        0.000333   \n",
       "\n",
       "  param_activation_name param_alpha param_batch_size param_hidden_layer_sizes  \\\n",
       "0            Leaky_relu         0.1               90                 (60, 60)   \n",
       "1               Softmax        0.01              260                   (100,)   \n",
       "2                  Tanh    0.215443               50               (100, 100)   \n",
       "3               Sigmoid    0.031623               80                (100, 50)   \n",
       "4            Leaky_relu    0.316228              430            (100, 50, 25)   \n",
       "\n",
       "  param_learning_rate param_optimizer_name  ... mean_test_score  \\\n",
       "0            0.000276                 Adam  ...        0.996755   \n",
       "1            0.034589                  SGD  ...        0.997656   \n",
       "2            0.002353                  SGD  ...        0.996214   \n",
       "3            0.003585                 Adam  ...        0.996755   \n",
       "4            0.000197                  SGD  ...        0.997296   \n",
       "\n",
       "  std_test_score rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0       0.001222               3            0.997521            1.000000   \n",
       "1       0.001223               1            1.000000            1.000000   \n",
       "2       0.000674               5            0.999324            1.000000   \n",
       "3       0.001222               3            0.998648            0.999775   \n",
       "4       0.000570               2            1.000000            1.000000   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.998873            1.000000            1.000000   \n",
       "1            1.000000            1.000000            1.000000   \n",
       "2            0.997747            0.997747            1.000000   \n",
       "3            1.000000            1.000000            0.999775   \n",
       "4            1.000000            1.000000            1.000000   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.999279         0.000981  \n",
       "1          1.000000         0.000000  \n",
       "2          0.998963         0.001024  \n",
       "3          0.999639         0.000506  \n",
       "4          1.000000         0.000000  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.randomized_search(\n",
    "    param_randomized=params['randomized'],\n",
    "    n_iter=n_iter,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=n_jobs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "407024f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 16,\n",
       " 'output_size': 1,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'activation_name': 'Softmax',\n",
       " 'loss': 'binary_cross_entropy',\n",
       " 'optimizer_name': 'SGD',\n",
       " 'learning_rate': 0.034588581370567514,\n",
       " 'batch_size': 260,\n",
       " 'weight_decay': 0.00010525948689799706,\n",
       " 'p_dropout': 0.2740878001587038,\n",
       " 'early_stopping': True,\n",
       " 'epochs': 200,\n",
       " 'patience': 10,\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e6b9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs: 100%|█| 200/200 [00:04<00:00, 46.24epo\n",
      "Semi-supervised training ; epochs: 100%|█| 200/200 [00:04<00:00, 47.\n",
      "/home/yanncauchepin/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1141: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_mlp.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fedc829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Set - Accuracy</th>\n",
       "      <td>0.997656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Set - Precision</th>\n",
       "      <td>0.997663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Set - Recall</th>\n",
       "      <td>0.997656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Set - F1</th>\n",
       "      <td>0.997651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Accuracy</th>\n",
       "      <td>0.995676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Precision</th>\n",
       "      <td>0.995699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Recall</th>\n",
       "      <td>0.995676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - F1</th>\n",
       "      <td>0.995658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Scores\n",
       "Train Set - Accuracy   0.997656\n",
       "Train Set - Precision  0.997663\n",
       "Train Set - Recall     0.997656\n",
       "Train Set - F1         0.997651\n",
       "Test Set - Accuracy    0.995676\n",
       "Test Set - Precision   0.995699\n",
       "Test Set - Recall      0.995676\n",
       "Test Set - F1          0.995658"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4212cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_mlp.receiver_operating_characteristics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac53482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_test = df_spark.sample(fraction=0.05, seed=30)\n",
    "df_test = df_test.dropna(subset=['loan_status'])\n",
    "features_collected = df_test.select(features).collect()\n",
    "X_test = np.array([list(feature) for feature in features_collected])\n",
    "target_collected = df_test.select('loan_status').collect()\n",
    "y_test = np.array([feature['loan_status'] for feature in target_collected])\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09624bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test Set - Accuracy</th>\n",
       "      <td>0.997056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Precision</th>\n",
       "      <td>0.997064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Recall</th>\n",
       "      <td>0.997056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - F1</th>\n",
       "      <td>0.997048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Scores\n",
       "Test Set - Accuracy   0.997056\n",
       "Test Set - Precision  0.997064\n",
       "Test Set - Recall     0.997056\n",
       "Test Set - F1         0.997048"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.model_performance_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13a3b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_value_collected = df_spark.sample(withReplacement=False, fraction=0.0001, seed=1).limit(1).collect()[0]\n",
    "single_value = np.array([value for key, value in single_value_collected.asDict().items() if key != 'loan_status']).reshape(1,-1).astype(float)\n",
    "single_value_target = np.array([value for key, value in single_value_collected.asDict().items() if key == 'loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7c7a098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0]]),\n",
       " array([[[False],\n",
       "         [False]]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.predict(single_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a02be47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_pymnt</th>\n",
       "      <td>63.022602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt</th>\n",
       "      <td>37.287211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <td>35.702657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rec_int</th>\n",
       "      <td>21.502705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <td>21.300784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>8.748340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <td>4.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <td>3.885814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <td>1.433799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_high</th>\n",
       "      <td>0.883805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <td>0.635252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_rate</th>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recoveries</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_rcnt_il</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      attribution\n",
       "loan_amnt              100.000000\n",
       "total_pymnt             63.022602\n",
       "funded_amnt             37.287211\n",
       "total_pymnt_inv         35.702657\n",
       "total_rec_int           21.502705\n",
       "total_rec_prncp         21.300784\n",
       "funded_amnt_inv          8.748340\n",
       "last_fico_range_low      4.000371\n",
       "last_fico_range_high     3.885814\n",
       "last_pymnt_amnt          1.433799\n",
       "fico_range_high          0.883805\n",
       "last_credit_pull_d       0.635252\n",
       "int_rate                 0.000416\n",
       "recoveries               0.000000\n",
       "mths_since_rcnt_il       0.000000\n",
       "mo_sin_old_rev_tl_op     0.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.compute_integrated_gradients(single_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00ed76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_mlp, 'lending_club_mlp_binary_classifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
