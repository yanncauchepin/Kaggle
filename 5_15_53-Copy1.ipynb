{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a499894c",
   "metadata": {},
   "source": [
    "## Yann Cauchepin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfce03",
   "metadata": {},
   "source": [
    "Hi, here is my documented jupyter notebook which respond to the test request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1b09e",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before even running the following script, please process by:\n",
    "\n",
    "- [ ] Installing the interested librairies. You can comment the next script to not bide your time.\n",
    "\n",
    "- [ ] Replacing dataset path toward your local repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a923e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install pyspark\n",
    "# !pip install catboost\n",
    "# !pip install shap\n",
    "# !pip install torch\n",
    "# !pip install captum\n",
    "# !pip install sklearn\n",
    "# !pip install mapie\n",
    "# !pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b006539",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/LCDataDictionary.xlsx\"\n",
    "data_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/Loan_status_2007-2020Q3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939e91c",
   "metadata": {},
   "source": [
    "Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957f17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(metadata_path, index_col=0)\n",
    "metadata = metadata.iloc[:-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f1740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ddf746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 20:18:20 WARN Utils: Your hostname, yanncauchepincomputer resolves to a loopback address: 127.0.1.1; using 192.168.43.208 instead (on interface wlp2s0)\n",
      "24/06/21 20:18:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/21 20:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 2:=====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 2925493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:====================================================>    (13 + 1) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LendingClubDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "print(f\"Number of data: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4751cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata features: 151\n",
      "Data features: 142\n",
      "Unknown data features: ['_c0', 'verification_status_joint', 'total_rev_hi_lim', 'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths', 'sec_app_collections_12_mths_ex_med'] (14)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metadata features: {len(metadata.index)}\")\n",
    "print(f\"Data features: {len(df_spark.columns)}\")\n",
    "\n",
    "outer_features = [feature for feature in df_spark.columns if feature not in metadata.index]\n",
    "\n",
    "print(f\"Unknown data features: {outer_features} ({len(outer_features)})\")\n",
    "df_spark = df_spark.drop(*outer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['grade', 'sub_grade']\n",
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83763e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [feature for feature in df_spark.columns if feature != 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3876cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distincts values: 12 - 4.10e-06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:================================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         loan_status|  count|\n",
      "+--------------------+-------+\n",
      "|          Fully Paid|1497783|\n",
      "|                NULL|      1|\n",
      "|     In Grace Period|  10028|\n",
      "|Does not meet the...|   1988|\n",
      "|         Charged Off| 362547|\n",
      "|  Late (31-120 days)|  16154|\n",
      "|             Current|1031016|\n",
      "|Does not meet the...|    761|\n",
      "|   Late (16-30 days)|   2719|\n",
      "|             Default|    433|\n",
      "|              Issued|   2062|\n",
      "|            Oct-2015|      1|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b9bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mapping = {\n",
    "    'Fully Paid': 0,\n",
    "    'Charged Off': 1,\n",
    "    'Current': np.nan,\n",
    "    'Late (31-120 days)': np.nan,\n",
    "    'In Grace Period': np.nan,\n",
    "    'Late (16-30 days)': np.nan,\n",
    "    'Issued': np.nan,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': np.nan,\n",
    "    'Does not meet the credit policy. Status:Charged Off': np.nan,\n",
    "    'Default': np.nan,\n",
    "    'Oct-2015': np.nan\n",
    "}\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df_spark = df_spark.withColumn(\"loan_status\", when(df_spark[\"loan_status\"] == \"Fully Paid\", 0)\n",
    "                   .when(df_spark[\"loan_status\"] == \"Charged Off\", 1)\n",
    "                   .otherwise(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a37f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.fillna({col: \"nan\" if df_spark.schema[col].dataType == 'string' else np.nan for col in all_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205dda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distincts values: 3 - 1.03e-06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|loan_status|  count|\n",
      "+-----------+-------+\n",
      "|        0.0|1497783|\n",
      "|        NaN|1065163|\n",
      "|        1.0| 362547|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:============================================>           (11 + 3) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83528a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('loan_amnt', 'int'),\n",
       " ('funded_amnt', 'int'),\n",
       " ('funded_amnt_inv', 'double'),\n",
       " ('term', 'string'),\n",
       " ('int_rate', 'string'),\n",
       " ('installment', 'double'),\n",
       " ('emp_title', 'string'),\n",
       " ('emp_length', 'string'),\n",
       " ('home_ownership', 'string'),\n",
       " ('annual_inc', 'string'),\n",
       " ('verification_status', 'string'),\n",
       " ('issue_d', 'string'),\n",
       " ('loan_status', 'double'),\n",
       " ('pymnt_plan', 'string'),\n",
       " ('url', 'string'),\n",
       " ('purpose', 'string'),\n",
       " ('title', 'string'),\n",
       " ('zip_code', 'string'),\n",
       " ('addr_state', 'string'),\n",
       " ('dti', 'string'),\n",
       " ('delinq_2yrs', 'double'),\n",
       " ('earliest_cr_line', 'string'),\n",
       " ('fico_range_low', 'string'),\n",
       " ('fico_range_high', 'int'),\n",
       " ('inq_last_6mths', 'int'),\n",
       " ('mths_since_last_delinq', 'int'),\n",
       " ('mths_since_last_record', 'int'),\n",
       " ('open_acc', 'int'),\n",
       " ('pub_rec', 'int'),\n",
       " ('revol_bal', 'int'),\n",
       " ('revol_util', 'string'),\n",
       " ('total_acc', 'string'),\n",
       " ('initial_list_status', 'string'),\n",
       " ('out_prncp', 'string'),\n",
       " ('out_prncp_inv', 'double'),\n",
       " ('total_pymnt', 'double'),\n",
       " ('total_pymnt_inv', 'double'),\n",
       " ('total_rec_prncp', 'double'),\n",
       " ('total_rec_int', 'double'),\n",
       " ('total_rec_late_fee', 'double'),\n",
       " ('recoveries', 'double'),\n",
       " ('collection_recovery_fee', 'double'),\n",
       " ('last_pymnt_d', 'string'),\n",
       " ('last_pymnt_amnt', 'string'),\n",
       " ('next_pymnt_d', 'string'),\n",
       " ('last_credit_pull_d', 'string'),\n",
       " ('last_fico_range_high', 'string'),\n",
       " ('last_fico_range_low', 'int'),\n",
       " ('collections_12_mths_ex_med', 'int'),\n",
       " ('mths_since_last_major_derog', 'int'),\n",
       " ('policy_code', 'int'),\n",
       " ('application_type', 'string'),\n",
       " ('annual_inc_joint', 'string'),\n",
       " ('dti_joint', 'double'),\n",
       " ('acc_now_delinq', 'int'),\n",
       " ('tot_coll_amt', 'int'),\n",
       " ('tot_cur_bal', 'int'),\n",
       " ('open_acc_6m', 'int'),\n",
       " ('open_act_il', 'int'),\n",
       " ('open_il_12m', 'int'),\n",
       " ('open_il_24m', 'int'),\n",
       " ('mths_since_rcnt_il', 'int'),\n",
       " ('total_bal_il', 'int'),\n",
       " ('il_util', 'int'),\n",
       " ('open_rv_12m', 'int'),\n",
       " ('open_rv_24m', 'int'),\n",
       " ('max_bal_bc', 'int'),\n",
       " ('all_util', 'int'),\n",
       " ('inq_fi', 'int'),\n",
       " ('total_cu_tl', 'int'),\n",
       " ('inq_last_12m', 'int'),\n",
       " ('acc_open_past_24mths', 'int'),\n",
       " ('avg_cur_bal', 'int'),\n",
       " ('bc_open_to_buy', 'int'),\n",
       " ('bc_util', 'double'),\n",
       " ('chargeoff_within_12_mths', 'double'),\n",
       " ('delinq_amnt', 'int'),\n",
       " ('mo_sin_old_il_acct', 'int'),\n",
       " ('mo_sin_old_rev_tl_op', 'int'),\n",
       " ('mo_sin_rcnt_rev_tl_op', 'int'),\n",
       " ('mo_sin_rcnt_tl', 'int'),\n",
       " ('mort_acc', 'int'),\n",
       " ('mths_since_recent_bc', 'int'),\n",
       " ('mths_since_recent_bc_dlq', 'int'),\n",
       " ('mths_since_recent_inq', 'int'),\n",
       " ('mths_since_recent_revol_delinq', 'int'),\n",
       " ('num_accts_ever_120_pd', 'int'),\n",
       " ('num_actv_bc_tl', 'int'),\n",
       " ('num_actv_rev_tl', 'int'),\n",
       " ('num_bc_sats', 'int'),\n",
       " ('num_bc_tl', 'int'),\n",
       " ('num_il_tl', 'int'),\n",
       " ('num_op_rev_tl', 'int'),\n",
       " ('num_rev_accts', 'int'),\n",
       " ('num_rev_tl_bal_gt_0', 'int'),\n",
       " ('num_sats', 'int'),\n",
       " ('num_tl_120dpd_2m', 'int'),\n",
       " ('num_tl_30dpd', 'int'),\n",
       " ('num_tl_90g_dpd_24m', 'int'),\n",
       " ('num_tl_op_past_12m', 'int'),\n",
       " ('pct_tl_nvr_dlq', 'double'),\n",
       " ('percent_bc_gt_75', 'double'),\n",
       " ('pub_rec_bankruptcies', 'int'),\n",
       " ('tax_liens', 'int'),\n",
       " ('tot_hi_cred_lim', 'int'),\n",
       " ('total_bal_ex_mort', 'int'),\n",
       " ('total_bc_limit', 'int'),\n",
       " ('total_il_high_credit_limit', 'int'),\n",
       " ('sec_app_open_act_il', 'int'),\n",
       " ('hardship_flag', 'string'),\n",
       " ('hardship_type', 'string'),\n",
       " ('hardship_reason', 'string'),\n",
       " ('hardship_status', 'string'),\n",
       " ('deferral_term', 'int'),\n",
       " ('hardship_amount', 'double'),\n",
       " ('hardship_start_date', 'string'),\n",
       " ('hardship_end_date', 'string'),\n",
       " ('payment_plan_start_date', 'string'),\n",
       " ('hardship_length', 'int'),\n",
       " ('hardship_dpd', 'int'),\n",
       " ('hardship_loan_status', 'string'),\n",
       " ('orig_projected_additional_accrued_interest', 'double'),\n",
       " ('hardship_payoff_balance_amount', 'double'),\n",
       " ('hardship_last_payment_amount', 'double'),\n",
       " ('debt_settlement_flag', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a4684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:========================>                                (6 + 8) / 14]\r",
      "\r",
      "[Stage 41:====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 145983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:================================================>       (12 + 2) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gb = df_spark.sample(fraction=0.05, seed=1)\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e255b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 92781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gb = df_gb.dropna(subset=['loan_status'])\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1680e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/21 20:19:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_gb = df_gb.select(all_features)\n",
    "features_collected_gb = features_gb.collect()\n",
    "target_gb = df_gb.select('loan_status')\n",
    "target_collected_gb = target_gb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d0f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb = np.array([list(feature) for feature in features_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee730a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb = np.array([feature['loan_status'] for feature in target_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2387d119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92781, 125)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d48cb9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92781,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a945bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [feature for (feature, dtype) in df_gb.dtypes if dtype=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6859dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.1319934\ttotal: 239ms\tremaining: 23.6s\n",
      "1:\tlearn: 0.0766225\ttotal: 341ms\tremaining: 16.7s\n",
      "2:\tlearn: 0.0388028\ttotal: 454ms\tremaining: 14.7s\n",
      "3:\tlearn: 0.0303303\ttotal: 529ms\tremaining: 12.7s\n",
      "4:\tlearn: 0.0235605\ttotal: 598ms\tremaining: 11.4s\n",
      "5:\tlearn: 0.0206519\ttotal: 667ms\tremaining: 10.4s\n",
      "6:\tlearn: 0.0195695\ttotal: 740ms\tremaining: 9.83s\n",
      "7:\tlearn: 0.0189217\ttotal: 806ms\tremaining: 9.27s\n",
      "8:\tlearn: 0.0175497\ttotal: 878ms\tremaining: 8.88s\n",
      "9:\tlearn: 0.0175093\ttotal: 946ms\tremaining: 8.52s\n",
      "10:\tlearn: 0.0167146\ttotal: 1.01s\tremaining: 8.19s\n",
      "11:\tlearn: 0.0158007\ttotal: 1.08s\tremaining: 7.95s\n",
      "12:\tlearn: 0.0157843\ttotal: 1.15s\tremaining: 7.73s\n",
      "13:\tlearn: 0.0123135\ttotal: 1.23s\tremaining: 7.53s\n",
      "14:\tlearn: 0.0123132\ttotal: 1.28s\tremaining: 7.24s\n",
      "15:\tlearn: 0.0120829\ttotal: 1.35s\tremaining: 7.08s\n",
      "16:\tlearn: 0.0108746\ttotal: 1.42s\tremaining: 6.92s\n",
      "17:\tlearn: 0.0108745\ttotal: 1.46s\tremaining: 6.67s\n",
      "18:\tlearn: 0.0108745\ttotal: 1.52s\tremaining: 6.47s\n",
      "19:\tlearn: 0.0108743\ttotal: 1.57s\tremaining: 6.3s\n",
      "20:\tlearn: 0.0106082\ttotal: 1.65s\tremaining: 6.21s\n",
      "21:\tlearn: 0.0105858\ttotal: 1.73s\tremaining: 6.12s\n",
      "22:\tlearn: 0.0101376\ttotal: 1.8s\tremaining: 6.03s\n",
      "23:\tlearn: 0.0099657\ttotal: 1.87s\tremaining: 5.92s\n",
      "24:\tlearn: 0.0097971\ttotal: 1.93s\tremaining: 5.8s\n",
      "25:\tlearn: 0.0097971\ttotal: 1.99s\tremaining: 5.66s\n",
      "26:\tlearn: 0.0090301\ttotal: 2.06s\tremaining: 5.57s\n",
      "27:\tlearn: 0.0088808\ttotal: 2.13s\tremaining: 5.48s\n",
      "28:\tlearn: 0.0088807\ttotal: 2.18s\tremaining: 5.33s\n",
      "29:\tlearn: 0.0088807\ttotal: 2.24s\tremaining: 5.22s\n",
      "30:\tlearn: 0.0088693\ttotal: 2.3s\tremaining: 5.13s\n",
      "31:\tlearn: 0.0088505\ttotal: 2.37s\tremaining: 5.03s\n",
      "32:\tlearn: 0.0088505\ttotal: 2.42s\tremaining: 4.92s\n",
      "33:\tlearn: 0.0088279\ttotal: 2.49s\tremaining: 4.84s\n",
      "34:\tlearn: 0.0088216\ttotal: 2.57s\tremaining: 4.77s\n",
      "35:\tlearn: 0.0087373\ttotal: 2.64s\tremaining: 4.7s\n",
      "36:\tlearn: 0.0078413\ttotal: 2.71s\tremaining: 4.62s\n",
      "37:\tlearn: 0.0078412\ttotal: 2.77s\tremaining: 4.52s\n",
      "38:\tlearn: 0.0066885\ttotal: 2.85s\tremaining: 4.45s\n",
      "39:\tlearn: 0.0066885\ttotal: 2.9s\tremaining: 4.35s\n",
      "40:\tlearn: 0.0065157\ttotal: 2.97s\tremaining: 4.27s\n",
      "41:\tlearn: 0.0064130\ttotal: 3.04s\tremaining: 4.2s\n",
      "42:\tlearn: 0.0063812\ttotal: 3.12s\tremaining: 4.14s\n",
      "43:\tlearn: 0.0063812\ttotal: 3.18s\tremaining: 4.04s\n",
      "44:\tlearn: 0.0063309\ttotal: 3.25s\tremaining: 3.98s\n",
      "45:\tlearn: 0.0062925\ttotal: 3.32s\tremaining: 3.9s\n",
      "46:\tlearn: 0.0061442\ttotal: 3.4s\tremaining: 3.84s\n",
      "47:\tlearn: 0.0061000\ttotal: 3.48s\tremaining: 3.77s\n",
      "48:\tlearn: 0.0060704\ttotal: 3.55s\tremaining: 3.69s\n",
      "49:\tlearn: 0.0060494\ttotal: 3.62s\tremaining: 3.62s\n",
      "50:\tlearn: 0.0060438\ttotal: 3.69s\tremaining: 3.54s\n",
      "51:\tlearn: 0.0056426\ttotal: 3.75s\tremaining: 3.47s\n",
      "52:\tlearn: 0.0055848\ttotal: 3.82s\tremaining: 3.39s\n",
      "53:\tlearn: 0.0055847\ttotal: 3.88s\tremaining: 3.3s\n",
      "54:\tlearn: 0.0055030\ttotal: 3.96s\tremaining: 3.24s\n",
      "55:\tlearn: 0.0054123\ttotal: 4.04s\tremaining: 3.17s\n",
      "56:\tlearn: 0.0050235\ttotal: 4.11s\tremaining: 3.1s\n",
      "57:\tlearn: 0.0047845\ttotal: 4.19s\tremaining: 3.03s\n",
      "58:\tlearn: 0.0047173\ttotal: 4.26s\tremaining: 2.96s\n",
      "59:\tlearn: 0.0047172\ttotal: 4.32s\tremaining: 2.88s\n",
      "60:\tlearn: 0.0046941\ttotal: 4.39s\tremaining: 2.81s\n",
      "61:\tlearn: 0.0046940\ttotal: 4.46s\tremaining: 2.73s\n",
      "62:\tlearn: 0.0046540\ttotal: 4.54s\tremaining: 2.66s\n",
      "63:\tlearn: 0.0046535\ttotal: 4.6s\tremaining: 2.59s\n",
      "64:\tlearn: 0.0046141\ttotal: 4.68s\tremaining: 2.52s\n",
      "65:\tlearn: 0.0045433\ttotal: 4.75s\tremaining: 2.44s\n",
      "66:\tlearn: 0.0044677\ttotal: 4.82s\tremaining: 2.38s\n",
      "67:\tlearn: 0.0044677\ttotal: 4.89s\tremaining: 2.3s\n",
      "68:\tlearn: 0.0043488\ttotal: 4.97s\tremaining: 2.23s\n",
      "69:\tlearn: 0.0042946\ttotal: 5.04s\tremaining: 2.16s\n",
      "70:\tlearn: 0.0042359\ttotal: 5.12s\tremaining: 2.09s\n",
      "71:\tlearn: 0.0042071\ttotal: 5.19s\tremaining: 2.02s\n",
      "72:\tlearn: 0.0041746\ttotal: 5.26s\tremaining: 1.95s\n",
      "73:\tlearn: 0.0041202\ttotal: 5.35s\tremaining: 1.88s\n",
      "74:\tlearn: 0.0041200\ttotal: 5.41s\tremaining: 1.8s\n",
      "75:\tlearn: 0.0040684\ttotal: 5.48s\tremaining: 1.73s\n",
      "76:\tlearn: 0.0040683\ttotal: 5.54s\tremaining: 1.66s\n",
      "77:\tlearn: 0.0040164\ttotal: 5.62s\tremaining: 1.58s\n",
      "78:\tlearn: 0.0039820\ttotal: 5.69s\tremaining: 1.51s\n",
      "79:\tlearn: 0.0039598\ttotal: 5.77s\tremaining: 1.44s\n",
      "80:\tlearn: 0.0039597\ttotal: 5.83s\tremaining: 1.37s\n",
      "81:\tlearn: 0.0039349\ttotal: 5.91s\tremaining: 1.3s\n",
      "82:\tlearn: 0.0039181\ttotal: 5.97s\tremaining: 1.22s\n",
      "83:\tlearn: 0.0038998\ttotal: 6.04s\tremaining: 1.15s\n",
      "84:\tlearn: 0.0038938\ttotal: 6.1s\tremaining: 1.08s\n",
      "85:\tlearn: 0.0038823\ttotal: 6.17s\tremaining: 1s\n",
      "86:\tlearn: 0.0038823\ttotal: 6.24s\tremaining: 932ms\n",
      "87:\tlearn: 0.0038822\ttotal: 6.3s\tremaining: 859ms\n",
      "88:\tlearn: 0.0038421\ttotal: 6.38s\tremaining: 788ms\n",
      "89:\tlearn: 0.0037379\ttotal: 6.45s\tremaining: 717ms\n",
      "90:\tlearn: 0.0037174\ttotal: 6.54s\tremaining: 647ms\n",
      "91:\tlearn: 0.0037173\ttotal: 6.63s\tremaining: 577ms\n",
      "92:\tlearn: 0.0036843\ttotal: 6.71s\tremaining: 505ms\n",
      "93:\tlearn: 0.0036596\ttotal: 6.78s\tremaining: 433ms\n",
      "94:\tlearn: 0.0036312\ttotal: 6.86s\tremaining: 361ms\n",
      "95:\tlearn: 0.0036311\ttotal: 6.92s\tremaining: 288ms\n",
      "96:\tlearn: 0.0036191\ttotal: 6.98s\tremaining: 216ms\n",
      "97:\tlearn: 0.0036191\ttotal: 7.04s\tremaining: 144ms\n",
      "98:\tlearn: 0.0036137\ttotal: 7.11s\tremaining: 71.8ms\n",
      "99:\tlearn: 0.0035804\ttotal: 7.19s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x79004a699c30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "pool = Pool(data=X_gb, label=y_gb, feature_names=all_features, cat_features=categorical_features)\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=100)\n",
    "catboost_model.fit(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc2cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(catboost_model)\n",
    "shap_values = explainer.shap_values(X_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d8587e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <td>417046.553348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recoveries</th>\n",
       "      <td>149158.378179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <td>135989.706385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>128005.130072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>93815.640670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <td>1.477193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_low</th>\n",
       "      <td>0.305097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_acc_6m</th>\n",
       "      <td>0.073228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verification_status</th>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_flag</th>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature_importance\n",
       "total_rec_prncp            417046.553348\n",
       "recoveries                 149158.378179\n",
       "last_fico_range_low        135989.706385\n",
       "loan_amnt                  128005.130072\n",
       "funded_amnt_inv             93815.640670\n",
       "...                                  ...\n",
       "hardship_loan_status            1.477193\n",
       "fico_range_low                  0.305097\n",
       "open_acc_6m                     0.073228\n",
       "verification_status             0.000302\n",
       "hardship_flag                   0.000125\n",
       "\n",
       "[106 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_over_feature = np.sum(np.abs(shap_values), axis=0)\n",
    "feature_importance = pd.DataFrame(data=sum_over_feature, index=all_features, columns=['feature_importance'])\n",
    "feature_importance = feature_importance[feature_importance['feature_importance']>0]\n",
    "feature_importance = feature_importance.sort_values(by='feature_importance', ascending=False)\n",
    "feature_importance.shape\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39cbd894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "      <th>cumulative_sum</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <td>417046.553348</td>\n",
       "      <td>4.170466e+05</td>\n",
       "      <td>0.302733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recoveries</th>\n",
       "      <td>149158.378179</td>\n",
       "      <td>5.662049e+05</td>\n",
       "      <td>0.411007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <td>135989.706385</td>\n",
       "      <td>7.021946e+05</td>\n",
       "      <td>0.509721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>128005.130072</td>\n",
       "      <td>8.301998e+05</td>\n",
       "      <td>0.602640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>93815.640670</td>\n",
       "      <td>9.240154e+05</td>\n",
       "      <td>0.670740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <td>1.477193</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_low</th>\n",
       "      <td>0.305097</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_acc_6m</th>\n",
       "      <td>0.073228</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verification_status</th>\n",
       "      <td>0.000302</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardship_flag</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>1.377605e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature_importance  cumulative_sum      rate\n",
       "total_rec_prncp            417046.553348    4.170466e+05  0.302733\n",
       "recoveries                 149158.378179    5.662049e+05  0.411007\n",
       "last_fico_range_low        135989.706385    7.021946e+05  0.509721\n",
       "loan_amnt                  128005.130072    8.301998e+05  0.602640\n",
       "funded_amnt_inv             93815.640670    9.240154e+05  0.670740\n",
       "...                                  ...             ...       ...\n",
       "hardship_loan_status            1.477193    1.377605e+06  1.000000\n",
       "fico_range_low                  0.305097    1.377605e+06  1.000000\n",
       "open_acc_6m                     0.073228    1.377605e+06  1.000000\n",
       "verification_status             0.000302    1.377605e+06  1.000000\n",
       "hardship_flag                   0.000125    1.377605e+06  1.000000\n",
       "\n",
       "[106 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance['cumulative_sum'] = feature_importance['feature_importance'].cumsum()\n",
    "total_sum = feature_importance['feature_importance'].sum()\n",
    "feature_importance['rate'] = feature_importance['cumulative_sum'] / total_sum\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bdd2694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "selected_features = feature_importance[feature_importance['rate'] < threshold].index\n",
    "selected_features\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d8b87fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_drop = [feature for feature in all_features if feature not in selected_features]\n",
    "features_to_drop\n",
    "len(features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8720722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"lending_club\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dbace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "to_float_udf = udf(to_float, FloatType())\n",
    "df_spark = df_spark.withColumn(\"last_fico_range_high\", to_float_udf(df_spark[\"last_fico_range_high\"]))\n",
    "df_spark = df_spark.withColumn(\"last_pymnt_amnt\", to_float_udf(df_spark[\"last_pymnt_amnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8de72519",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_pymnt_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_pymnt_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_pymnt_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_pymnt_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_pymnt_d LIKE 'May-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_pymnt_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_pymnt_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_pymnt_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_pymnt_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_pymnt_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_pymnt_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_pymnt_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_pymnt_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_pymnt_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_pymnt_d_num', 'last_pymnt_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a41c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_credit_pull_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_credit_pull_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_credit_pull_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_credit_pull_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_credit_pull_d LIKE 'May-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_credit_pull_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_credit_pull_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_credit_pull_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_credit_pull_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_credit_pull_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_credit_pull_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_credit_pull_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_credit_pull_d_num', 'last_credit_pull_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ddc7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(\"last_pymnt_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_pymnt_d_num', 'last_pymnt_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a3645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_float_rate = udf(lambda x: float(x.replace('%', '')) / 100, FloatType())\n",
    "df_spark = df_spark.withColumn('int_rate', convert_to_float_rate(df_spark['int_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab30efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "numerical_selected_features = [feature for (feature, dtype) in df_spark.dtypes if (dtype!='string' and feature !='loan_status')]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_selected_features,\n",
    "    outputCols=numerical_selected_features\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "df_spark = imputer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a751f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan\n",
    "# for feature, dtype in df_spark.dtypes:\n",
    "#     print(\"====================================\")\n",
    "#     print(f\"FEATURE: {feature}\")\n",
    "#     if dtype=='string':\n",
    "#         value_counts = df_spark.groupBy(feature).count()\n",
    "#         value_rate = value_counts.count() / df_spark.count()\n",
    "#         print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "#         value_counts.show()\n",
    "#     nan_count = df_spark.filter(df_spark[feature].isNull() | isnan(df_spark[feature])).count()\n",
    "#     nan_rate = nan_count / df_spark.count()\n",
    "#     print(f\"{nan_count} NaN - {nan_rate:.2e} %\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f79120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 14518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:================================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled data: 9247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:====================================================>   (13 + 1) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df_spark.sample(fraction=0.005, seed=1)\n",
    "print(f\"Number of data: {df.count()}\")\n",
    "df_labeled = df.dropna(subset=['loan_status'])\n",
    "print(f\"Number of labeled data: {df_labeled.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b745349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:========================================>               (10 + 4) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN data: 5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "df_nan = df.filter(isnan(df['loan_status']))\n",
    "print(f\"Number of NaN data: {df_nan.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "756ac9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in list(df_spark.columns) if feature != 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8eb7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_collected = df_labeled.select(features).collect()\n",
    "target_collected = df_labeled.select('loan_status').collect()\n",
    "X_labeled = np.array([list(feature) for feature in features_collected])\n",
    "y_labeled = np.array([feature['loan_status'] for feature in target_collected])\n",
    "y_labeled = y_labeled.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6636f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_collected = df_nan.select(features).collect()\n",
    "X_unlabeled = np.array([list(feature) for feature in features_collected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b383f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = np.unique(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f3b5568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from captum.attr import IntegratedGradients\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from mapie.classification import MapieClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_sizes, activation_name, p_dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        if isinstance(self.hidden_layer_sizes, str):\n",
    "            self.hidden_layer_sizes = eval(self.hidden_layer_sizes)\n",
    "        self.activation_name = activation_name\n",
    "        self.p_dropout = p_dropout\n",
    "        if activation_name == \"Relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_name == \"Sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation_name == \"Softmax\":\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        elif activation_name == \"Tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_name == \"Leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation: {self.activation_name}')\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_size, self.hidden_layer_sizes[0]))\n",
    "        layers.append(self.activation)\n",
    "        for i in range(len(self.hidden_layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(self.hidden_layer_sizes[i], self.hidden_layer_sizes[i + 1]))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.Dropout(p=self.p_dropout))\n",
    "        layers.append(nn.Linear(self.hidden_layer_sizes[-1], self.output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward_proba(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.float()\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MLPEstimatorSklearn():\n",
    "    def __init__(self, **params):\n",
    "        self.input_size = params.get(\"input_size\")\n",
    "        self.output_size = params.get(\"output_size\")\n",
    "        self.hidden_layer_sizes = params.get(\"hidden_layer_sizes\", (60, 60))\n",
    "        self.activation_name = params.get(\"activation_name\", \"Relu\")\n",
    "        self.loss = params.get(\"loss\", \"binary_cross_entropy\")\n",
    "        self.optimizer_name = params.get(\"optimizer_name\", \"Adam\")\n",
    "        self.learning_rate = params.get(\"learning_rate\", 1e-3)\n",
    "        self.batch_size = params.get(\"batch_size\", 50)\n",
    "        self.weight_decay = params.get(\"weight_decay\", 0)\n",
    "        self.p_dropout = params.get(\"p_dropout\", 0.2)\n",
    "        self.early_stopping = params.get(\"early_stopping\", True)\n",
    "        self.epochs = params.get(\"epochs\", 200)\n",
    "        self.patience = params.get(\"patience\", 10)\n",
    "        self.verbose = params.get(\"verbose\", True)\n",
    "        self.classes_ = classes_\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = MLP(self.input_size, self.output_size, self.hidden_layer_sizes, self.activation_name, self.p_dropout).to(self.device)\n",
    "\n",
    "        if self.loss == \"binary_cross_entropy\":\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss: {self.loss}\")\n",
    "\n",
    "        if self.optimizer_name == \"SGD\":\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9, weight_decay=self.weight_decay)\n",
    "        elif self.optimizer_name == \"Adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
    "            \n",
    "    def next_batch(self, inputs, targets, batchSize):\n",
    "        inputs_tensor = torch.from_numpy(inputs).float()\n",
    "        targets_tensor = torch.from_numpy(targets).float().unsqueeze(1)\n",
    "        for i in range(0, inputs_tensor.shape[0], batchSize):\n",
    "            yield (inputs_tensor[i:i + batchSize], targets_tensor[i:i + batchSize])\n",
    "\n",
    "    def augment_data(self, X_unlabeled, noise_level=0.1):\n",
    "        noise = noise_level * torch.randn_like(X_unlabeled)\n",
    "        return X_unlabeled + noise\n",
    "            \n",
    "    def fit(self, X, y, X_unlabeled=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        running_losses_1 = list()\n",
    "        if self.early_stopping:\n",
    "            best_loss = float('inf')\n",
    "            count = 0\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1 = tqdm(range(self.epochs), desc=\"Supervised training ; epochs\", unit=\"epoch\")\n",
    "        else:\n",
    "            epoch_iterator_1 = range(self.epochs)\n",
    "        for epoch in epoch_iterator_1:\n",
    "            samples = 0\n",
    "            train_loss = 0.0\n",
    "            self.model.train(True)\n",
    "            for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                batchX = batchX.to(self.device)\n",
    "                batchY = batchY.to(self.device)\n",
    "                batchY.requires_grad = True\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batchX)\n",
    "                loss = self.criterion(outputs, batchY)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                samples += batchY.size(0)\n",
    "            running_loss = train_loss / samples\n",
    "            running_losses_1.append(running_loss)\n",
    "            if self.verbose:\n",
    "                epoch_iterator_1.set_postfix(train_loss=running_loss)\n",
    "            if self.early_stopping:\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    count = 0\n",
    "                else:\n",
    "                    count += 1\n",
    "                if count >= self.patience:\n",
    "                    break\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1.close()\n",
    "        self.running_losses_1 = [loss for loss in running_losses_1 if loss <= best_loss]\n",
    "        if X_unlabeled is not None:\n",
    "            best_loss = float('inf')\n",
    "            running_losses_2 = list()\n",
    "            X_unlabeled = torch.from_numpy(X_unlabeled).float()\n",
    "            augmented_X_unlabeled = self.augment_data(X_unlabeled)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = self.model(X_unlabeled)\n",
    "                pseudo_labels = (pseudo_labels > 0.5).float().squeeze()\n",
    "            \n",
    "            X_combined = torch.cat((torch.from_numpy(X).float(), augmented_X_unlabeled.cpu()), 0).to(self.device)\n",
    "            y_combined = torch.cat((torch.from_numpy(y).float().squeeze(), pseudo_labels), 0).to(self.device)\n",
    "\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2 = tqdm(range(self.epochs), desc=\"Semi-supervised training ; epochs\", unit=\"epoch\")\n",
    "            else:\n",
    "                epoch_iterator_2 = range(self.epochs)\n",
    "            for epoch in epoch_iterator_2:\n",
    "                samples = 0\n",
    "                train_loss = 0.0\n",
    "                self.model.train(True)\n",
    "                dataset = TensorDataset(X_combined, y_combined)\n",
    "                for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                    batchX = batchX.to(self.device)\n",
    "                    batchY = batchY.to(self.device)\n",
    "                    batchY.requires_grad = True\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(batchX)\n",
    "                    loss = self.criterion(outputs, batchY)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                    samples += batchY.size(0)\n",
    "                running_loss = train_loss / samples\n",
    "                running_losses_2.append(running_loss)\n",
    "                if self.verbose:\n",
    "                    epoch_iterator_2.set_postfix(train_loss=running_loss)\n",
    "                if self.early_stopping:\n",
    "                    if running_loss < best_loss:\n",
    "                        best_loss = running_loss\n",
    "                        count = 0\n",
    "                    else:\n",
    "                        count += 1\n",
    "                    if count >= self.patience:\n",
    "                        break\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2.close()\n",
    "            self.running_losses_2 = [loss for loss in running_losses_2 if loss <= best_loss]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_pred = self.model.forward(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            y_pred = y_pred.detach().numpy()\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_proba = self.model.forward_proba(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_proba = y_proba.cpu().detach().numpy().astype(float)\n",
    "        else:\n",
    "            y_proba = y_proba.detach().numpy().astype(float)\n",
    "        y_proba = y_proba.squeeze().astype(np.float32)\n",
    "        return np.column_stack((1 - y_proba, y_proba))\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            \"input_size\": self.input_size,\n",
    "            \"output_size\": self.output_size,\n",
    "            \"hidden_layer_sizes\": self.hidden_layer_sizes,\n",
    "            \"activation_name\": self.activation_name,\n",
    "            \"loss\": self.loss,\n",
    "            \"optimizer_name\": self.optimizer_name,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"p_dropout\": self.p_dropout,\n",
    "            \"early_stopping\": self.early_stopping,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"patience\": self.patience,\n",
    "            \"verbose\": self.verbose\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def get_mlp(self):\n",
    "        return self.model\n",
    "    \n",
    "class MLPBinaryClassifier():\n",
    "    def __init__(self, X, y, split_test, X_unlabeled=None, **params):\n",
    "        self.model = MLPEstimatorSklearn(**params)\n",
    "        self.X = X\n",
    "        self.X_unlabeled = X_unlabeled\n",
    "        self.y = y\n",
    "        \n",
    "        self.y = MLPBinaryClassifier.float_to_class(self.y).ravel()\n",
    "        \n",
    "        self.split_test = split_test\n",
    "        self.split_data()\n",
    "        \n",
    "        self.standardize(self.X_train_cal)\n",
    "        self.X_train_standard = self.standardize_X(self.X_train)\n",
    "        self.X_cal_standard = self.standardize_X(self.X_cal)\n",
    "        if isinstance(self.X_unlabeled, np.ndarray):\n",
    "            self.X_unlabeled_standard = self.standardize_X(self.X_unlabeled)\n",
    "        else :\n",
    "            self.X_unlabeled_standard = None\n",
    "        self.y_train_standard = self.y_train\n",
    "        self.y_cal_standard = self.y_cal.reshape(-1,1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def float_to_class(y):\n",
    "        threshold = 0.5\n",
    "        return (y >= threshold).astype(int)\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.X_train_cal, self.X_test, self.y_train_cal, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=self.split_test, shuffle=True, random_state=1, stratify=self.y)\n",
    "        self.X_train, self.X_cal, self.y_train, self.y_cal = train_test_split(\n",
    "            self.X_train_cal, self.y_train_cal, test_size=0.25, shuffle=True, random_state=1, stratify=self.y_train_cal)\n",
    "\n",
    "    def standardize(self, X):\n",
    "        self.scaler_X_train = StandardScaler()\n",
    "        self.scaler_X_train.fit(X)         \n",
    "\n",
    "\n",
    "    def standardize_X(self, X):\n",
    "        X_new = self.scaler_X_train.transform(X)\n",
    "        return X_new\n",
    "    \n",
    "\n",
    "\n",
    "    def bayes_search(self, param_bayes, n_iter, n_points=1, cv=5, scoring='accuracy',\n",
    "                 verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        bayes_search = BayesSearchCV(self.model, param_bayes, n_iter=n_iter,\n",
    "                                     n_points=n_points, cv=cv, scoring=scoring,\n",
    "                                     verbose=verbose, return_train_score=True,\n",
    "                                     n_jobs=n_jobs, random_state=1)\n",
    "        bayes_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "        self.model = bayes_search.best_estimator_\n",
    "        print(f'Best hyperparameters bayes search : {bayes_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def randomized_search(self, param_randomized, n_iter, cv=5, scoring='accuracy',\n",
    "                      verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        randomized_search = RandomizedSearchCV(self.model, param_randomized,\n",
    "                                               n_iter=n_iter, cv=cv, scoring=scoring,\n",
    "                                               verbose=verbose, return_train_score=True,\n",
    "                                               n_jobs=n_jobs, random_state=1)\n",
    "        randomized_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(randomized_search.cv_results_)\n",
    "        self.model = randomized_search.best_estimator_\n",
    "        print(f'Best hyperparameters randomized search : {randomized_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def fit(self, method=\"lac\"):\n",
    "        self.model.fit(self.X_train_standard, self.y_train_standard, self.X_unlabeled_standard)\n",
    "        self.model_mapie = MapieClassifier(estimator=self.model, cv=\"prefit\", method=method)\n",
    "        self.model_mapie.fit(self.X_cal_standard, self.y_cal_standard)\n",
    "\n",
    "    def predict(self, X, alpha=0.05):\n",
    "        X_standard = self.standardize_X(X)\n",
    "        y_pred, y_ps = self.model_mapie.predict(X_standard, alpha=alpha)\n",
    "        return y_pred, y_ps\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(metric, y_true, y_pred):\n",
    "        y_pred = MLPBinaryClassifier.float_to_class(y_pred)\n",
    "        accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "        precision = metrics.precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "        metrics_dict = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "        }\n",
    "        if metric != 'all':\n",
    "            metrics_dict = {metric: metrics_dict[metric]}\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "    def model_performance(self, metric='all'):\n",
    "        y_pred_train, _ = self.predict(self.X_train)\n",
    "        scores_train = MLPBinaryClassifier.compute_metrics(metric, self.y_train, y_pred_train)\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, self.y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_train.items():\n",
    "            data['Train Set - '+key] = [value]\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def model_performance_test(self, X_test, y_test, metric='all'):\n",
    "        y_pred_test, _ = self.predict(X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def receiver_operating_characteristics(self):\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(self.y_test, y_pred_test)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.title(\"Receiver Operating Characteristics\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.show()\n",
    "\n",
    "    def compute_integrated_gradients(self, X, baseline=None, steps=50):\n",
    "        def preprocess_input(X):\n",
    "            return torch.tensor(X, dtype=torch.float32)\n",
    "        input_tensor = preprocess_input(X)\n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(input_tensor)\n",
    "        integrated_gradients = IntegratedGradients(self.model.get_mlp())\n",
    "        attributions = integrated_gradients.attribute(input_tensor, baseline, target=0, n_steps=steps)\n",
    "        attributions_df = pd.DataFrame(attributions.cpu().detach().numpy(), columns=features)\n",
    "        avg_attributions = attributions_df.mean(axis=0)\n",
    "        avg_abs_attributions = avg_attributions.abs()\n",
    "        def custom_minmax_scaler(data, feature_range=(0, 100)):\n",
    "            min_val = np.min(data)\n",
    "            max_val = np.max(data)\n",
    "            if max_val - min_val == 0:\n",
    "                return np.zeros_like(data) if feature_range[0] == 0 else np.full_like(data, feature_range[0])\n",
    "            scale = (feature_range[1] - feature_range[0]) / (max_val - min_val)\n",
    "            min_range = feature_range[0]\n",
    "            scaled_data = scale * (data - min_val) + min_range\n",
    "            return scaled_data\n",
    "        normalized_data = custom_minmax_scaler(avg_abs_attributions.values.reshape(-1, 1)).astype(float)\n",
    "        np.set_printoptions(suppress=True, precision=2)\n",
    "        normalized_attributions = pd.DataFrame(normalized_data, columns=['attribution'], index=features)\n",
    "        sorted_attributions = normalized_attributions.sort_values(by=\"attribution\", ascending=False)\n",
    "        return sorted_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ab236126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "from skopt.space import Real\n",
    "\n",
    "params = {    \n",
    "    \"init\" : {\n",
    "        \"input_size\" : len(features),\n",
    "        \"output_size\" : 1,\n",
    "        \"hidden_layer_sizes\" : (60,60),\n",
    "        \"activation_name\" : \"Relu\",\n",
    "        \"optimizer_name\" : \"Adam\",\n",
    "        \"learning_rate\" : 1e-3,\n",
    "        \"batch_size\" : 50,\n",
    "        \"weight_decay\" : 0,\n",
    "        \"p_dropout\" : 0.3,\n",
    "        \"loss\" : \"binary_cross_entropy\",\n",
    "        \"early_stopping\" : True,\n",
    "        \"epochs\" : 200,\n",
    "        \"patience\" : 10,\n",
    "        \"verbose\" : True\n",
    "    },\n",
    "    \"randomized\": {\n",
    "        \"hidden_layer_sizes\" : [(10,),(50,),(100,),(10,10),(50,50),(60,60),(100,50),(100,100),(100,50,25)],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : loguniform(1e-5, 1),\n",
    "        \"p_dropout\" : uniform(0, 0.4)   \n",
    "    },\n",
    "    \"bayes\": {\n",
    "        \"hidden_layer_sizes\" : [\"(10,)\",\"(50,)\",\"(100,)\",\"(10,10)\",\"(50,50)\",\"(60,60)\",\"(100,50)\",\"(100,100)\",\"(100,50,25)\"],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : Real(1e-5, 1, prior='log-uniform'),\n",
    "        \"p_dropout\" : Real(0, 0.4, prior='uniform')\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mlp = MLPBinaryClassifier(X=X_labeled, y=y_labeled, X_unlabeled=X_unlabeled, split_test=0.2, **params[\"init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02818466",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "n_points=1\n",
    "cv=5\n",
    "scoring='accuracy'\n",
    "verbose=3\n",
    "n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5845bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp.bayes_search(\n",
    "#     param_bayes=params['bayes'],\n",
    "#     n_iter=n_iter,\n",
    "#     n_points=n_points,\n",
    "#     cv=cv,\n",
    "#     scoring=scoring,\n",
    "#     n_jobs=n_jobs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d63527ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  66%|██████▌   | 131/200 [00:08<00:04, 15.84epoch/s, train_loss=3.43e-6]\n",
      "Supervised training ; epochs:  62%|██████▎   | 125/200 [00:08<00:04, 15.07epoch/s, train_loss=4.41e-6]\n",
      "Supervised training ; epochs:  32%|███▎      | 65/200 [00:08<00:16,  8.02epoch/s, train_loss=9.19e-6]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.998) total time=   8.3s\n",
      "[CV 3/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.996) total time=   8.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  90%|█████████ | 180/200 [00:09<00:01, 19.08epoch/s, train_loss=1.28e-6]\n",
      "Supervised training ; epochs:   7%|▋         | 14/200 [00:01<00:12, 14.90epoch/s, train_loss=0.000123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.997) total time=   9.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  14%|█▍        | 29/200 [00:02<00:09, 17.85epoch/s, train_loss=5.25e-5] \n",
      "Supervised training ; epochs:  38%|███▊      | 77/200 [00:10<00:17,  7.22epoch/s, train_loss=4.42e-5]\n",
      "Supervised training ; epochs:  16%|█▌        | 32/200 [00:02<00:08, 18.98epoch/s, train_loss=4.57e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=0.999, test=0.996) total time=  10.5s\n",
      "[CV 2/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=0.999, test=0.994) total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  36%|███▌      | 72/200 [00:10<00:19,  6.59epoch/s, train_loss=1.38e-5]\n",
      "Supervised training ; epochs:  38%|███▊      | 76/200 [00:11<00:20,  5.92epoch/s, train_loss=1.35e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=1.000, test=0.996) total time=  10.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  40%|███▉      | 79/200 [00:11<00:17,  6.74epoch/s, train_loss=1.05e-5]]\n",
      "Supervised training ; epochs:  34%|███▍      | 69/200 [00:03<00:06, 20.80epoch/s, train_loss=1.47e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=0.999, test=0.997) total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  44%|████▍     | 88/200 [00:12<00:15,  7.04epoch/s, train_loss=1.14e-5]]\n",
      "Supervised training ; epochs:  42%|████▏     | 84/200 [00:04<00:05, 20.96epoch/s, train_loss=1.35e-5]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Leaky_relu, alpha=0.1, batch_size=90, hidden_layer_sizes=(60, 60), learning_rate=0.0002755926764027377, optimizer_name=Adam, p_dropout=0.15863229091841047, weight_decay=0.0008700690210600529;, score=(train=1.000, test=0.997) total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  72%|███████▏  | 144/200 [00:08<00:03, 17.35epoch/s, train_loss=2.91e-6]\n",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]7epoch/s, train_loss=0.00011] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.997) total time=   8.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  75%|███████▌  | 150/200 [00:08<00:02, 17.17epoch/s, train_loss=3.09e-6]\n",
      "Supervised training ; epochs:  12%|█▎        | 25/200 [00:06<00:49,  3.53epoch/s, train_loss=9.87e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation_name=Softmax, alpha=0.01, batch_size=260, hidden_layer_sizes=(100,), learning_rate=0.034588581370567514, optimizer_name=SGD, p_dropout=0.2740878001587038, weight_decay=0.00010525948689799706;, score=(train=1.000, test=0.998) total time=   8.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  34%|███▍      | 68/200 [00:15<00:30,  4.34epoch/s, train_loss=1.02e-5] \n",
      "Supervised training ; epochs:  30%|███       | 60/200 [00:08<00:18,  7.44epoch/s, train_loss=1.22e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=1.000, test=0.996) total time=  15.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Supervised training ; epochs:  30%|███       | 61/200 [00:14<00:30,  4.59epoch/s, train_loss=1.11e-5]\r",
      "Supervised training ; epochs:  31%|███       | 62/200 [00:14<00:29,  4.60epoch/s, train_loss=1.11e-5]\r",
      "Supervised training ; epochs:  29%|██▉       | 58/200 [00:08<00:20,  6.88epoch/s, train_loss=1.06e-5]\r",
      "Supervised training ; epochs:  30%|██▉       | 59/200 [00:08<00:24,  5.83epoch/s, train_loss=1.06e-5]\r",
      "Supervised training ; epochs:  29%|██▉       | 58/200 [00:13<00:29,  4.83epoch/s, train_loss=6.83e-6]\r",
      "Supervised training ; epochs:  30%|██▉       | 59/200 [00:13<00:29,  4.83epoch/s, train_loss=6.83e-6]\r",
      "Supervised training ; epochs:  30%|███       | 60/200 [00:08<00:18,  7.44epoch/s, train_loss=1.8e-5] \r",
      "Supervised training ; epochs:  30%|███       | 61/200 [00:08<00:18,  7.44epoch/s, train_loss=1.8e-5]\r",
      "Supervised training ; epochs:  36%|███▌      | 72/200 [00:12<00:27,  4.63epoch/s, train_loss=2.75e-5]\r",
      "Supervised training ; epochs:  36%|███▋      | 73/200 [00:12<00:26,  4.76epoch/s, train_loss=2.75e-5]\r",
      "Supervised training ; epochs:  30%|███       | 60/200 [00:14<00:35,  3.89epoch/s, train_loss=5.38e-6]\r",
      "Supervised training ; epochs:  30%|███       | 61/200 [00:14<00:34,  4.02epoch/s, train_loss=5.38e-6]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00518]\r",
      "Supervised training ; epochs:   0%|          | 1/200 [00:00<00:46,  4.28epoch/s, train_loss=0.00518]\r",
      "Supervised training ; epochs:  29%|██▉       | 58/200 [00:14<00:38,  3.69epoch/s, train_loss=0.000154]\r",
      "Supervised training ; epochs:  29%|██▉       | 58/200 [00:14<00:35,  4.04epoch/s, train_loss=0.000154]\n",
      "\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]\r",
      "Supervised training ; epochs:  30%|███       | 61/200 [00:08<00:18,  7.44epoch/s, train_loss=1.19e-5]\r",
      "Supervised training ; epochs:  31%|███       | 62/200 [00:08<00:18,  7.44epoch/s, train_loss=1.19e-5]\r",
      "Supervised training ; epochs:  30%|██▉       | 59/200 [00:08<00:24,  5.83epoch/s, train_loss=8.3e-6] \r",
      "Supervised training ; epochs:  30%|███       | 60/200 [00:08<00:23,  5.88epoch/s, train_loss=8.3e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=0.999, test=0.996) total time=  14.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  38%|███▊      | 77/200 [00:13<00:21,  5.67epoch/s, train_loss=8.51e-6] \n",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:07, 25.48epoch/s, train_loss=0.001]23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation_name=Sigmoid, alpha=0.03162277660168379, batch_size=80, hidden_layer_sizes=(100, 50), learning_rate=0.003584739875476995, optimizer_name=Adam, p_dropout=0.357842665401539, weight_decay=2.6620797194541587e-05;, score=(train=0.999, test=0.996) total time=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  34%|███▍      | 68/200 [00:15<00:30,  4.35epoch/s, train_loss=3.58e-5]]\n",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:07, 26.12epoch/s, train_loss=0.00066] ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=0.999, test=0.997) total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Supervised training ; epochs:  36%|███▌      | 72/200 [00:16<00:27,  4.74epoch/s, train_loss=1.14e-5]\r",
      "Supervised training ; epochs:  38%|███▊      | 76/200 [00:10<00:17,  7.24epoch/s, train_loss=1.18e-5]\r",
      "Supervised training ; epochs:  36%|███▌      | 72/200 [00:16<00:29,  4.27epoch/s, train_loss=1.14e-5]\n",
      "\r",
      "Supervised training ; epochs:  38%|███▊      | 77/200 [00:10<00:16,  7.25epoch/s, train_loss=1.18e-5]\r",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:07, 26.12epoch/s, train_loss=0.000479]\r",
      "Supervised training ; epochs:   3%|▎         | 6/200 [00:00<00:06, 27.99epoch/s, train_loss=0.000479]\r",
      "Supervised training ; epochs:  16%|█▋        | 33/200 [00:01<00:07, 22.90epoch/s, train_loss=3.69e-5]\r",
      "Supervised training ; epochs:   6%|▌         | 12/200 [00:02<00:31,  5.89epoch/s, train_loss=0.000177]\r",
      "Supervised training ; epochs:   6%|▋         | 13/200 [00:02<00:32,  5.72epoch/s, train_loss=0.000177]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s]\r",
      "Supervised training ; epochs:   3%|▎         | 6/200 [00:00<00:06, 27.99epoch/s, train_loss=0.000371]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00159]\r",
      "Supervised training ; epochs:  16%|█▋        | 33/200 [00:01<00:07, 22.90epoch/s, train_loss=3.22e-5]\r",
      "Supervised training ; epochs:   3%|▎         | 6/200 [00:00<00:06, 27.99epoch/s, train_loss=0.0003]  \r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.0013] \r",
      "Supervised training ; epochs:  37%|███▋      | 74/200 [00:10<00:18,  6.87epoch/s, train_loss=4.34e-6]\r",
      "Supervised training ; epochs:  38%|███▊      | 75/200 [00:10<00:18,  6.91epoch/s, train_loss=4.34e-6]\r",
      "Supervised training ; epochs:   3%|▎         | 6/200 [00:00<00:06, 27.99epoch/s, train_loss=0.000258]\r",
      "Supervised training ; epochs:   4%|▍         | 9/200 [00:00<00:06, 28.46epoch/s, train_loss=0.000258]\r",
      "Supervised training ; epochs:  16%|█▋        | 33/200 [00:01<00:07, 22.90epoch/s, train_loss=2.98e-5]\r",
      "Supervised training ; epochs:  18%|█▊        | 36/200 [00:01<00:07, 21.42epoch/s, train_loss=2.98e-5]\r",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00106]\r",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:07, 26.95epoch/s, train_loss=0.00106]\r",
      "Supervised training ; epochs:  38%|███▊      | 77/200 [00:11<00:16,  7.25epoch/s, train_loss=9.54e-6]\r",
      "Supervised training ; epochs:  39%|███▉      | 78/200 [00:11<00:17,  7.16epoch/s, train_loss=9.54e-6]\r",
      "Supervised training ; epochs:   4%|▍         | 9/200 [00:00<00:06, 28.46epoch/s, train_loss=0.000218]\r",
      "Supervised training ; epochs:   6%|▋         | 13/200 [00:02<00:31,  5.87epoch/s, train_loss=0.000171]\r",
      "Supervised training ; epochs:   7%|▋         | 14/200 [00:02<00:35,  5.27epoch/s, train_loss=0.000171]\r",
      "Supervised training ; epochs:  18%|█▊        | 36/200 [00:01<00:07, 21.42epoch/s, train_loss=2.97e-5]\r",
      "Supervised training ; epochs:   2%|▏         | 3/200 [00:00<00:07, 26.95epoch/s, train_loss=0.000841]\r",
      "Supervised training ; epochs:   4%|▍         | 9/200 [00:00<00:06, 28.46epoch/s, train_loss=0.000194]\r",
      "Supervised training ; epochs:  18%|█▊        | 36/200 [00:01<00:07, 21.42epoch/s, train_loss=2.88e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=1.000, test=0.996) total time=  16.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  36%|███▌      | 71/200 [00:17<00:31,  4.06epoch/s, train_loss=6.68e-6]]\n",
      "Supervised training ; epochs:  40%|████      | 80/200 [00:11<00:17,  6.94epoch/s, train_loss=2.6e-5] ]\n",
      "Supervised training ; epochs:   0%|          | 0/200 [00:00<?, ?epoch/s, train_loss=0.00172]=9.7e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation_name=Tanh, alpha=0.21544346900318823, batch_size=50, hidden_layer_sizes=(100, 100), learning_rate=0.002352959348061621, optimizer_name=SGD, p_dropout=0.05615477543809351, weight_decay=9.783797277120768e-05;, score=(train=1.000, test=0.995) total time=  17.5s\n",
      "[CV 2/5] END activation_name=Sigmoid, alpha=0.03162277660168379, batch_size=80, hidden_layer_sizes=(100, 50), learning_rate=0.003584739875476995, optimizer_name=Adam, p_dropout=0.357842665401539, weight_decay=2.6620797194541587e-05;, score=(train=1.000, test=0.998) total time=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  47%|████▋     | 94/200 [00:13<00:15,  6.77epoch/s, train_loss=1.28e-5]]\n",
      "Supervised training ; epochs:  68%|██████▊   | 137/200 [00:05<00:02, 24.99epoch/s, train_loss=3.41e-6]\n",
      "Supervised training ; epochs:  86%|████████▋ | 173/200 [00:07<00:01, 23.93epoch/s, train_loss=2.18e-6]\n",
      "Supervised training ; epochs:  59%|█████▉    | 118/200 [00:05<00:03, 21.91epoch/s, train_loss=5.25e-6]\n",
      "Supervised training ; epochs:  86%|████████▋ | 173/200 [00:05<00:00, 29.05epoch/s, train_loss=1.69e-6]\n",
      "Supervised training ; epochs: 100%|██████████| 200/200 [00:07<00:00, 28.46epoch/s, train_loss=1.55e-6]\n",
      "Supervised training ; epochs:  37%|███▋      | 74/200 [00:10<00:17,  7.03epoch/s, train_loss=1.58e-5]\n",
      "Supervised training ; epochs:  56%|█████▌    | 112/200 [00:13<00:10,  8.48epoch/s, train_loss=5.19e-6]\n",
      "Supervised training ; epochs:  37%|▎| 74/200 [00:07<00:12, 10.28epoc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters randomized search : {'activation_name': 'Sigmoid', 'alpha': 0.03162277660168379, 'batch_size': 80, 'hidden_layer_sizes': (100, 50), 'learning_rate': 0.003584739875476995, 'optimizer_name': 'Adam', 'p_dropout': 0.357842665401539, 'weight_decay': 2.6620797194541587e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation_name</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer_name</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.266422</td>\n",
       "      <td>0.753257</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>Leaky_relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>90</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996214</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.999549</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.615996</td>\n",
       "      <td>0.452661</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>Softmax</td>\n",
       "      <td>0.01</td>\n",
       "      <td>260</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997476</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.011008</td>\n",
       "      <td>1.087260</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>0.215443</td>\n",
       "      <td>50</td>\n",
       "      <td>(100, 100)</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996395</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.548113</td>\n",
       "      <td>1.295377</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>80</td>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997476</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.218502</td>\n",
       "      <td>0.770526</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>Leaky_relu</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>430</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996935</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      11.266422      0.753257         0.003060        0.000338   \n",
       "1       8.615996      0.452661         0.002532        0.000259   \n",
       "2      16.011008      1.087260         0.006216        0.005905   \n",
       "3      12.548113      1.295377         0.002144        0.000523   \n",
       "4       6.218502      0.770526         0.002417        0.000847   \n",
       "\n",
       "  param_activation_name param_alpha param_batch_size param_hidden_layer_sizes  \\\n",
       "0            Leaky_relu         0.1               90                 (60, 60)   \n",
       "1               Softmax        0.01              260                   (100,)   \n",
       "2                  Tanh    0.215443               50               (100, 100)   \n",
       "3               Sigmoid    0.031623               80                (100, 50)   \n",
       "4            Leaky_relu    0.316228              430            (100, 50, 25)   \n",
       "\n",
       "  param_learning_rate param_optimizer_name  ... mean_test_score  \\\n",
       "0            0.000276                 Adam  ...        0.996214   \n",
       "1            0.034589                  SGD  ...        0.997476   \n",
       "2            0.002353                  SGD  ...        0.996395   \n",
       "3            0.003585                 Adam  ...        0.997476   \n",
       "4            0.000197                  SGD  ...        0.996935   \n",
       "\n",
       "  std_test_score rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0       0.001323               5            1.000000            0.999324   \n",
       "1       0.000675               2            1.000000            1.000000   \n",
       "2       0.000569               4            1.000000            1.000000   \n",
       "3       0.001051               1            0.999324            0.999775   \n",
       "4       0.001348               3            1.000000            1.000000   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.999775            0.999324            0.999324   \n",
       "1            1.000000            1.000000            1.000000   \n",
       "2            1.000000            0.999099            0.998873   \n",
       "3            1.000000            1.000000            0.999099   \n",
       "4            1.000000            1.000000            1.000000   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.999549         0.000285  \n",
       "1          1.000000         0.000000  \n",
       "2          0.999594         0.000502  \n",
       "3          0.999639         0.000366  \n",
       "4          1.000000         0.000000  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.randomized_search(\n",
    "    param_randomized=params['randomized'],\n",
    "    n_iter=n_iter,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=n_jobs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "407024f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 16,\n",
       " 'output_size': 1,\n",
       " 'hidden_layer_sizes': (100, 50),\n",
       " 'activation_name': 'Sigmoid',\n",
       " 'loss': 'binary_cross_entropy',\n",
       " 'optimizer_name': 'Adam',\n",
       " 'learning_rate': 0.003584739875476995,\n",
       " 'batch_size': 80,\n",
       " 'weight_decay': 2.6620797194541587e-05,\n",
       " 'p_dropout': 0.357842665401539,\n",
       " 'early_stopping': True,\n",
       " 'epochs': 200,\n",
       " 'patience': 10,\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4e6b9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supervised training ; epochs:  24%|▏| 48/200 [00:06<00:20,  7.56epoc\n",
      "Semi-supervised training ; epochs:  18%|▏| 35/200 [00:04<00:21,  7.5\n",
      "/home/yanncauchepin/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1141: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_mlp.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fedc829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Set - Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Set - Precision</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Set - Recall</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Set - F1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Accuracy</th>\n",
       "      <td>0.995135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Precision</th>\n",
       "      <td>0.995145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Recall</th>\n",
       "      <td>0.995135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - F1</th>\n",
       "      <td>0.995118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Scores\n",
       "Train Set - Accuracy   1.000000\n",
       "Train Set - Precision  1.000000\n",
       "Train Set - Recall     1.000000\n",
       "Train Set - F1         1.000000\n",
       "Test Set - Accuracy    0.995135\n",
       "Test Set - Precision   0.995145\n",
       "Test Set - Recall      0.995135\n",
       "Test Set - F1          0.995118"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e4212cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_mlp.receiver_operating_characteristics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac53482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_test = df_spark.sample(fraction=0.05, seed=30)\n",
    "df_test = df_test.dropna(subset=['loan_status'])\n",
    "features_collected = df_test.select(features).collect()\n",
    "X_test = np.array([list(feature) for feature in features_collected])\n",
    "target_collected = df_test.select('loan_status').collect()\n",
    "y_test = np.array([feature['loan_status'] for feature in target_collected])\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09624bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test Set - Accuracy</th>\n",
       "      <td>0.997390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Precision</th>\n",
       "      <td>0.997393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - Recall</th>\n",
       "      <td>0.997390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Set - F1</th>\n",
       "      <td>0.997385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Scores\n",
       "Test Set - Accuracy   0.997390\n",
       "Test Set - Precision  0.997393\n",
       "Test Set - Recall     0.997390\n",
       "Test Set - F1         0.997385"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.model_performance_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13a3b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_value_collected = df_spark.sample(withReplacement=False, fraction=0.0001, seed=1).limit(1).collect()[0]\n",
    "single_value = np.array([value for key, value in single_value_collected.asDict().items() if key != 'loan_status']).reshape(1,-1).astype(float)\n",
    "single_value_target = np.array([value for key, value in single_value_collected.asDict().items() if key == 'loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f7c7a098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0]]),\n",
       " array([[[ True],\n",
       "         [False]]]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.predict(single_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a02be47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_pymnt</th>\n",
       "      <td>80.078953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <td>79.085749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>67.841225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>63.756837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt</th>\n",
       "      <td>55.498672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rec_int</th>\n",
       "      <td>12.877730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <td>1.728181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <td>1.578478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <td>1.010099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_high</th>\n",
       "      <td>0.962472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <td>0.009081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_rate</th>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recoveries</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_rcnt_il</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      attribution\n",
       "total_rec_prncp        100.000000\n",
       "total_pymnt             80.078953\n",
       "total_pymnt_inv         79.085749\n",
       "loan_amnt               67.841225\n",
       "funded_amnt_inv         63.756837\n",
       "funded_amnt             55.498672\n",
       "total_rec_int           12.877730\n",
       "last_fico_range_high     1.728181\n",
       "last_pymnt_amnt          1.578478\n",
       "last_credit_pull_d       1.010099\n",
       "fico_range_high          0.962472\n",
       "last_fico_range_low      0.009081\n",
       "int_rate                 0.000119\n",
       "recoveries               0.000000\n",
       "mths_since_rcnt_il       0.000000\n",
       "mo_sin_old_rev_tl_op     0.000000"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.compute_integrated_gradients(single_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
